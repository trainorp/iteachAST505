{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0) Instructions:\n",
    "Please complete the workbook below. Some of the calculations are already ready to be \"run\". However, please read the text carefully to find questions that you should answer for credit. Remember that to answer a question in text, you click \"insert\", then \"insert cell below\", switch the input from \"code\" to \"markdown\", type your answer, and finally click the run button to set your text in stone. In a few questions, you will need to do some calculations on your own. Hint: for these calculations you can copy, paste, and modify code that is above the calculation that you need to do. You may work by yourself or in groups of 2. Please remember to put your name on top, remember to save the workbook, and remember to upload to Canvas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please run the code below to load the *tidyverse* set of packages and change the default image size to 5 x 4. Please ignore the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "options(repr.plot.width = 5, repr.plot.height = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Analysis of Variance Part 1: The overall $F$ test\n",
    "Here we will dicuss comparing means between three or more populations. As an example we will analyze the weights of chickens that were fed various types of feed. This data was published in the journal *Biometrika* in 1948. Below, we will load the data using the \"data\" command, and we will also do some sampling from this dataset (this involves the \"group_by\" and \"slice\") commands. Feel free to run and ignore this part. Basically, the original data has unbalanced sample sizes (some $n$'s are more than others) and we have not yet discussed how to deal with this case. So we will take 10 observations from each population:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data(chickwts)\n",
    "chickwts <- chickwts %>% group_by(feed) %>% slice(1:10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now that the data is available to us, let's use the \"head\" function to take a look at the first 6 observations: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(chickwts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From that you can observe that one of the types of feeds for these chickens was \"horsebean\". Let's take a look at the other types of feed that were included in this experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table(chickwts$feed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we discuss conducting an Analysis of Variance, let's take a look at the weight data from one of the populations of chickens (that was fed linseed)\". Let's determine if these weights appear to be approximately normal. Please run the code below to make a Q-Q plot of the data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qqnorm(chickwts$weight[chickwts$feed == \"linseed\"])\n",
    "qqline(chickwts$weight[chickwts$feed == \"linseed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question for you.** Based on the above Q-Q plot, would you say that the data is approximately normally distributed? Please answer below in a markdown cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question for you**. Imagine that you want to determine if there is evidence that the chicken weights depends on the type of feed that is given to the chickens. Below, please explain what your null and alternative hypotheses would be for which we will use an overall $F$ test. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we formally conduct an ANOVA $F$ test, let's take a look at the distribution of the weights for each sample with a different type of feed. Below we will make a boxplot that has feed on the horizontal axis (x), weight on the vertical axis (y), and is also collored (fill) inside by the feed type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(chickwts, aes(x = feed, fill = feed, y = weight)) + geom_boxplot() + theme_bw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our ANOVA $F$ test involves population means we might want to add the means within each sample to the boxplot. Below, we will add the means as red diamonds. Also in this version, we will add the \"alpha = .5\" to the geom_boxplot layer to make the color more translucent. Beautiful art!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question for you.** Based on the boxplot with the means shown above, do you think we will find evidence that the mean weight of chickens does depend on the type of feed? Please answer below in a new markdown cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1) Conducting an ANOVA $F$ test \"by hand\"\n",
    "Let's assume that we want to conduct an ANOVA to determine if we have sufficient evidence $\\alpha = 0.01$, that the mean chicken weights differ by feed type. We have a long process to conduct an ANOVA $F$ test \"by hand\". Once we have specified our hypotheses (you have already done this in section #2), we will need to determine a critical value from an $F$ distribution that will give us a rejection rule, and we will need to determine our $F$ test statistic.  Let's start by determining the $F$ test statistic. For this we will need to determine the within-sample sum of squares (SSW) and the sum of squares between samples (SSB). Let's start with SSB first. \n",
    "\n",
    "Remember that for SSB, we use the following: $\\sum_{j = 1}^k n_j (\\bar{y}_{j.} - \\bar{y}_{..} )^2$\n",
    "\n",
    "So we will need to calculate $\\bar{y}_{j.}$ which is the mean weight for each type of feed $j$. The aggregate function in R allows us to do *something* by a group variable. Here that *something* is determining the mean of \"weight\" and the group variable is feed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate(weight ~ feed, data = chickwts, mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we can see the means from each group. Now, let's save this set of means as a new \"data.frame\" called \"groupMeans\": "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupMeans <- aggregate(weight ~ feed, data = chickwts, mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to determine $\\bar{y}_{..}$, we will need to take the mean of the $\\bar{y}_{j.}$'s: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(groupMeans$weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to calculate the $\\bar{y}_{j.} - \\bar{y}_{..} $'s we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupMeans$weight - mean(groupMeans$weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so now let's save these $\\bar{y}_{j.} - \\bar{y}_{..}$ values as a new column in our \"data.frame\" that had the group means. This will allows us to square them and sum them up later on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupMeans$meanMinusGrandMean <- groupMeans$weight - mean(groupMeans$weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, let's pause here and see what this \"data.frame\" looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now let's add a new column that will have the $(\\bar{y}_{j.} - \\bar{y}_{..} )^2$ values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupMeans$meanMinusGrandMeanSq <- groupMeans$meanMinusGrandMean^2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's take a look at this \"data.frame\" now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have everything that we need to compute $\\text{SSB} = \\sum_{j = 1}^k n_j (\\bar{y}_{j.} - \\bar{y}_{..} )^2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(10 * groupMeans$meanMinusGrandMeanSq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And also it's mean square:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(10 * groupMeans$meanMinusGrandMeanSq) / (6-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have SSB, let's calculate SSW. To do this we will use the formula: $(n_1 - 1) s_1^2 + (n_2 - 1) s_2^2 + (n_3 - 1) s_3^2 + ... + (n_k - 1) s_k^2$. We can use the \"aggregate\" function as we did before to calculate the sample variance for each group: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate(weight ~ feed, data = chickwts, FUN = var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, let's save the above \"data.frame\" that has all of the sample variances (of chicken weights) by feed as \"varByGroup\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varByGroup <- aggregate(weight ~ feed, data = chickwts, FUN = var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, no to find SSW, we need to multiply each one of these sample variances by the relevant sample size minus 1 (here this is 9 for all groups) and add them all together. We do this below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(9 * varByGroup$weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question for you.** What are the values of SSB and SSW? Please answer below in a new markdown cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next step prior to determining our ANOVA $F$ statistic will be to determine the mean squares, $s_B^2$ and $s_W^2$, by using $s_B^2 = \\frac{\\text{SSB}}{k-1}$ and $s_W^2 = \\frac{SSW}{N-k}$. Below, in separate cells we will calculate each. We will then save them as \"s2b\" and \"s2w\". Please run the next three cells to do this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "225392.6 / (6 - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "167892.8 / (60 - 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2b <- 225392.6 / (6 - 1)\n",
    "s2w <- 167892.8 / (60 - 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now that we have these mean squares we can determine our ANOVA $F$-test statistic: $F^* = s^2_B / s^2_W$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2b / s2w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to determine a critical value, to see if we have evidence that supports rejecting $H_0$. We are going to look for a quantile from the $F$ distribution that has $k-1$ and $N-k$ degrees of freedom. You will notice that these degrees of freedom are the same integers that we divided SSB and SSW by in order to find the mean squares. As an example of how to do this, let's find the quantile from an $F$ distribution with 4 and 40 degrees of freedom that would give us a quantile for a right-tailed probability of 0.025 (please note this is not the quantile we need for our ANOVA):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qf(.025, df1 = 4, df2 = 40, lower.tail = FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question for you.** Determine the correct critical value from an $F$ distribution that we need for conducting our overall ANOVA $F$-test. You should copy, paste, and modify the code from above to find this critical value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question for you**. Based on your observed test statistic and the critical value corresponding to this test setup, what is your conclusion? In terms of the original problem involving feed type and chicken weights, what is your conclusion? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will determine the level of significance of the test that we have conducted. To do this we will need to determine $P(F \\geq F^*)$. As an example calculation, below you will find $P(F \\geq 10)$ from an $F$ distribution with 4 and 40 degrees of freedom: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf(10, 4, 40, lower.tail = FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question for you.** Copy, paste, and modify the code from above in a new code cell below to determine the level of significance of the overall ANOVA $F$-test that we have conducted. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2) Conducting an ANOVA $F$-test using software"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conducting an ANOVA $F$-test using R is super easy and very fun. We will use the \"aov\" function. The first argument to this function is called the formula. This has the following form: \"Response variable ~ Group variable\". So for us, this will be \"weight ~ feed\" to examine if there are differences in the population means of chicken weights by feed type. the next argument is \"data\" which tells R which \"data.frame\" contains these two variables. We wrap the \"aov\" function in another function called \"summary\". What this function does is provide us with a nice summary table from the ANOVA $F$-test, showing us SSB, SSW, the mean squares, the $F^*$ test statistic, and the p-value for the test. Please run the code to see this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(aov(weight ~ feed, data = chickwts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Back to the chickens (multiple comparisons)\n",
    "In the last section, we conducted an Analysis of Variance for determining if there was evidence that *at least* one of the population means of chicken weights differed by feed type. Now the all important question is which of the means are different from the others, or more specifically we will want to test which pairs of means are different from one another. Before we get to making these comparisons, let's make a boxplot to reorient ourselves!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to make a boxplot showing the distribution of chicken weights by feed type. The means are shown as red triangles: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(chickwts, aes(x = feed, fill = feed, y = weight)) + geom_boxplot(alpha = .5) + \n",
    "  stat_summary(fun.y = mean, colour = \"darkred\", geom = \"point\", shape = 18, size = 3, show.legend = FALSE) +\n",
    "  theme_bw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question for you** For which of the pairs of groups do you think the difference in the means will be greatest? For which of the pairs of groups do you think the differences in the means will be smallest? Please answer these questions in a new markdown cell below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returning to the ANOVA, the hypotheses being tested were $H_0: \\mu_1 = \\mu_2 = ... = \\mu_p$ versus $H_a:$ At least one $\\mu_j$ is not equal to the others. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, let's generate the ANOVA again using R. We will save this object that contains our results as \"anova1\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anova1 <- aov(weight ~ feed, data = chickwts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to get our ANOVA table we can use the \"summary\" function, which will return the sum of squares, the mean squares, the $F^*$ test statistic, and the p-value from the $F$ test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(anova1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question for you**. In the above output you find a p-value, labeled as \"Pr(>F)\". What does this p-value represent? What does it tell you about our original research question as to whether the choice of feed impacts chicken weights? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1) Pairwise comparisons\n",
    "Okay, since we have observed significant evidence that at least one of the population means is different from the others, let's now compare each of the means (pairwise comparisons). We can start by taking a look at each of the sample means within each group (we have done this earlier in the lab): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate(weight ~ feed, data = chickwts, mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save this set of means by group as \"groupMeans\" as we will need them later to make pairwise comparisons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupMeans <- aggregate(weight ~ feed, data = chickwts, mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1) Fisher's Least Significant Difference\n",
    "The first method for making pairwise comparisons that we will discus is the Fisher's Least Significant Difference (LSD) approach. This approach does not preserve the experimentwise type I error probability below a specified $\\alpha$. In other words, if you set $\\alpha = 0.05$, for the whole set of pairwise comparisons that you make, the experimentwise type I error proabability will be greater than $\\alpha$. However, this approach is superior to doing pairwise t-tests as we have learned in earlier lectures when we were comparing the means of two populations; This is due to the fact that the within group mean square, aka $s_W^2$, is a better estimate of the variance (assuming equal variances) than the square of the pooled standard deviation estimate that we learned earlier, $s_p$.\n",
    "\n",
    "**Question for you.** In practice, what does \"the experimentwise type I error proabability will be greater than $\\alpha$\" mean? Please explain what it means for both hypothesis tests and confidence intervals. Please answer this question below in a new markdown cell with one or a few sentances. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start off by comparing the means between chicken weights for the chickens fed horsebean versus casein. We will determine a point estimate and confidence interval for $\\mu_{\\text{horsebean}} - \\mu_{\\text{casein}}$. For the point estimate we simply contrast the sample means. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "160.2 - 326.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to estimate a 95% confidence interval for $\\mu_{\\text{horsebean}} - \\mu_{\\text{casein}}$, we will use $\\bar{y}_1 - \\bar{y}_2 \\pm t_{\\alpha / 2, N-k} \\sqrt{\\frac{SSW}{N-k}\\left(\\frac{1}{n_1} + \\frac{1}{n_2} \\right)}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(160.2 - 326.8) - qt(0.025, df = 60 - 6, lower.tail = FALSE) * sqrt(3109 * (1/10 + 1/10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(160.2 - 326.8) + qt(0.025, df = 60 - 6, lower.tail = FALSE) * sqrt(3109 * (1/10 + 1/10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question for you.** Repeating the process we have completed abouve for determining a point estimate and 95% confidence interval for $\\mu_{\\text{horsebean}} - \\mu_{\\text{casein}}$, determine a point estimate and confidence interval for $\\mu_{\\text{soybean}} - \\mu_{\\text{linseed}}$. You may copy, paste, and modify the code from above into new cells to do this. At the end please add a new markdown cell to report what the point estimate and 95% confidence interval are. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to this point, we have made two pairwise comparisons of population means. Later we will use software to make all 15 pairwise comparisons. But for now, let's discuss statistical hypothesis tests. To make pairwise hypothesis tests comparing the means of two of the populations we can use the LSD method. Remeber that this method will not preserve the experimental Type I error rate at the specified $\\alpha$. \n",
    "\n",
    "Let's start by testing $H_0: \\mu_{\\text{horsebean}} - \\mu_{\\text{casein}}= 0$ versus $H_a:\\mu_{\\text{horsebean}} - \\mu_{\\text{casein}} \\neq 0$. To do this we will need to specify an $\\alpha$ (let's use 0.05), a test statistic, and a rejection rule. The test statistic is: $t^*=\\frac{\\bar{y}_1 - \\bar{y}_2}{\\sqrt{\\frac{SSW}{N-k}\\left(\\frac{1}{n_1} + \\frac{1}{n_2} \\right) }}$.\n",
    "\n",
    "**Question for you**. Please state the rejection rule for this test below in a new markdown cell. Please actually find the crictical value (you may use R to do this of course!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so now to calculate our test statistic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(160.2 - 326.8) / sqrt(3109 * (1/10 + 1/10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question for you**. What is the conclusion of this invidual pairwise comparison? Please answer below in a new markdown cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to determine the level of significance, or p-value, we will determine $2\\times P(t \\geq |t^*|)$ using a $t$ distribution with $N-k$ degrees of freedom. First let's find $\\times P(t \\geq |t^*|)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt((160.2 - 326.8) / sqrt(3109 * (1/10 + 1/10)), df = 60 - 6, lower.tail = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's find $2 \\times P(t \\geq |t^*|)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2 * pt((160.2 - 326.8) / sqrt(3109 * (1/10 + 1/10)), df = 60 - 6, lower.tail = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question for you**. Using the Fisher's LSD method, conduct a statistical hypothesis test to determine if there is evidence that the population mean weights are different for chickens fed linsead versus horsebean. You may copy and paste the codes from above into new code cells. After you conduct the test, please state your conclusion in a new markdown cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to do make all of the pairwise comparisons at once, we can use the \"PostHocTest\" function from the \"DescTools\" package. The *DescTools* package is the package that destroyed last week's lab since it wouldn't install. I took the appropriate code out from this package that we will need. Please run the code block below to make the \"PostHocTest\" function available to us. If you are using R at home, you can use this function by installing the \"DescTools\" package on your computer.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PostHocTest <- function(x, which = NULL, method = c(\"hsd\", \"bonferroni\", \n",
    "    \"lsd\", \"scheffe\", \"newmankeuls\", \"duncan\"), \n",
    "    conf.level = 0.95, ordered = FALSE, ...) \n",
    "{\n",
    "    method <- match.arg(method)\n",
    "    if (method == \"scheffe\") {\n",
    "        out <- ScheffeTest(x = x, which = which, conf.level = conf.level, \n",
    "            ...)\n",
    "    }\n",
    "    else {\n",
    "        mm <- model.tables(x, \"means\")\n",
    "        if (is.null(mm$n)) \n",
    "            stop(\"no factors in the fitted model\")\n",
    "        tabs <- mm$tables[-1L]\n",
    "        if (is.null(which)) \n",
    "            which <- seq_along(tabs)\n",
    "        tabs <- tabs[which]\n",
    "        nn <- mm$n[names(tabs)]\n",
    "        nn_na <- is.na(nn)\n",
    "        if (all(nn_na)) \n",
    "            stop(\"'which' specified no factors\")\n",
    "        if (any(nn_na)) {\n",
    "            warning(\"'which' specified some non-factors which will be dropped\")\n",
    "            tabs <- tabs[!nn_na]\n",
    "            nn <- nn[!nn_na]\n",
    "        }\n",
    "        out <- setNames(vector(\"list\", length(tabs)), names(tabs))\n",
    "        MSE <- sum(x$residuals^2)/x$df.residual\n",
    "        for (nm in names(tabs)) {\n",
    "            tab <- tabs[[nm]]\n",
    "            means <- as.vector(tab)\n",
    "            nms <- if (length(d <- dim(tab)) > 1L) {\n",
    "                dn <- dimnames(tab)\n",
    "                apply(do.call(\"expand.grid\", dn), 1L, paste, \n",
    "                  collapse = \":\")\n",
    "            }\n",
    "            else names(tab)\n",
    "            n <- nn[[nm]]\n",
    "            if (length(n) < length(means)) \n",
    "                n <- rep.int(n, length(means))\n",
    "            if (method %in% c(\"hsd\", \"newmankeuls\", \n",
    "                \"duncan\") & as.logical(ordered)) {\n",
    "                ord <- order(means)\n",
    "                means <- means[ord]\n",
    "                n <- n[ord]\n",
    "                if (!is.null(nms)) \n",
    "                  nms <- nms[ord]\n",
    "            }\n",
    "            center <- outer(means, means, \"-\")\n",
    "            keep <- lower.tri(center)\n",
    "            center <- center[keep]\n",
    "            switch(method, bonferroni = {\n",
    "                width <- qt(1 - (1 - conf.level)/(length(means) * \n",
    "                  (length(means) - 1)), x$df.residual) * sqrt(MSE * \n",
    "                  outer(1/n, 1/n, \"+\"))[keep]\n",
    "                est <- center/sqrt(MSE * outer(1/n, 1/n, \"+\")[keep])\n",
    "                pvals <- pmin(2 * pt(abs(est), df = x$df.residual, \n",
    "                  lower.tail = FALSE) * ((length(means)^2 - length(means))/2), \n",
    "                  1)\n",
    "                method.str <- \"Bonferroni\"\n",
    "            }, lsd = {\n",
    "                width <- qt(1 - (1 - conf.level)/2, x$df.residual) * \n",
    "                  sqrt(MSE * outer(1/n, 1/n, \"+\"))[keep]\n",
    "                est <- center/sqrt(MSE * outer(1/n, 1/n, \"+\")[keep])\n",
    "                pvals <- 2 * pt(abs(est), df = x$df.residual, \n",
    "                  lower.tail = FALSE)\n",
    "                method.str <- \"Fisher LSD\"\n",
    "            }, hsd = {\n",
    "                width <- qtukey(conf.level, length(means), x$df.residual) * \n",
    "                  sqrt((MSE/2) * outer(1/n, 1/n, \"+\"))[keep]\n",
    "                est <- center/(sqrt((MSE/2) * outer(1/n, 1/n, \n",
    "                  \"+\"))[keep])\n",
    "                pvals <- ptukey(abs(est), length(means), x$df.residual, \n",
    "                  lower.tail = FALSE)\n",
    "                method.str <- \"Tukey HSD\"\n",
    "            }, newmankeuls = {\n",
    "                nmean <- (abs(outer(rank(means), rank(means), \n",
    "                  \"-\")) + 1)[keep]\n",
    "                width <- qtukey(conf.level, nmean, x$df.residual) * \n",
    "                  sqrt((MSE/2) * outer(1/n, 1/n, \"+\"))[keep]\n",
    "                est <- center/(sqrt((MSE/2) * outer(1/n, 1/n, \n",
    "                  \"+\"))[keep])\n",
    "                pvals <- ptukey(abs(est), nmean, x$df.residual, \n",
    "                  lower.tail = FALSE)\n",
    "                method.str <- \"Newman-Keuls\"\n",
    "            }, duncan = {\n",
    "                nmean <- (abs(outer(rank(means), rank(means), \n",
    "                  \"-\")) + 1)[keep]\n",
    "                width <- qtukey(conf.level^(nmean - 1), nmean, \n",
    "                  x$df.residual) * sqrt((MSE/2) * outer(1/n, \n",
    "                  1/n, \"+\"))[keep]\n",
    "                est <- center/(sqrt((MSE/2) * outer(1/n, 1/n, \n",
    "                  \"+\"))[keep])\n",
    "                pvals <- 1 - (1 - ptukey(abs(est), nmean, x$df.residual, \n",
    "                  lower.tail = FALSE))^(1/(nmean - 1))\n",
    "                method.str <- \"Duncan's new multiple range test\"\n",
    "            }, dunnett = {\n",
    "                method.str <- \"Dunnett\"\n",
    "            }, scottknott = {\n",
    "                method.str <- \"Scott Knott\"\n",
    "            }, waller = {\n",
    "                method.str <- \"Waller\"\n",
    "            }, gabriel = {\n",
    "                method.str <- \"Gabriel\"\n",
    "            })\n",
    "            if (!is.na(conf.level)) {\n",
    "                dnames <- list(NULL, c(\"diff\", \"lwr.ci\", \n",
    "                  \"upr.ci\", \"pval\"))\n",
    "                if (!is.null(nms)) \n",
    "                  dnames[[1L]] <- outer(nms, nms, paste, sep = \"-\")[keep]\n",
    "                out[[nm]] <- array(c(center, center - width, \n",
    "                  center + width, pvals), c(length(width), 4L), \n",
    "                  dnames)\n",
    "            }\n",
    "            else {\n",
    "                out[[nm]] <- matrix(NA, nrow = length(means), \n",
    "                  ncol = length(means))\n",
    "                out[[nm]][lower.tri(out[[nm]], diag = FALSE)] <- pvals\n",
    "                dimnames(out[[nm]]) <- list(nms, nms)\n",
    "                out[[nm]] <- out[[nm]][-1, -ncol(out[[nm]])]\n",
    "            }\n",
    "        }\n",
    "        class(out) <- c(\"PostHocTest\")\n",
    "        attr(out, \"orig.call\") <- x$call\n",
    "        attr(out, \"conf.level\") <- conf.level\n",
    "        attr(out, \"ordered\") <- ordered\n",
    "        attr(out, \"method\") <- method.str\n",
    "        attr(out, \"method.str\") <- gettextf(\"\\n  Posthoc multiple comparisons of means : %s \\n\", \n",
    "            attr(out, \"method\"))\n",
    "    }\n",
    "    return(out)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PostHocTest function takes the ANOVA object that we created earlier as the first argument, and then requires you to specify the method for making pairwise comparisons. In our case, we will use 'method = \"lsd\"'. The code below takes out anova object \"anova1\" and uses it to make pairwise comparisons using the Fisher's LSD method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2 * 15 * pt((267.4 - 211.0) / sqrt(3109 * (1/10 + 1/10)), df = 60 - 6, lower.tail = FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(anova1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "267.4 - 211.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PostHocTest(anova1, method = \"lsd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.2) The Bonferroni procedure\n",
    "The problem with the Fisher's LSD procedure is that it does not preserve the experimentwise Type I error probability at the specified $\\alpha$. The most straightforward (and naive) way to ensure that the Type I error probability is actually maintained at $\\alpha$ over the whole set of pairwise comparisons made is to simply divide $\\alpha$ by the number of pairwise comparisons. We will call this new value $\\alpha_i$ meaning if there are $m$ pairwise comparisons we set $\\alpha_i = \\alpha_E / m$. \n",
    "\n",
    "For our chicken weights example, there are 6 different types of feeds. So to make all of the possible pairwise comparisons of mean chicken weights by group we will have 15 tests. 15 is the number of ways that you can choose a set of two objects from a set of 6. As the total number of populations evaluated in the ANOVA increases, it can be hard to count all of the pairwise comparisons. We can use the \"choose\" function in R to tell us how many there are. For example to count the number of ways to choose a set of two objects out of 6 we do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PostHocTest(anova1, method = \"lsd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choose(6, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, before we determine confidence intervals for the difference in population means and conduct hypothesis tests to determine for which comparisons we have evidence of a difference in means we need to find $\\alpha_i = \\alpha_E / m$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.05 / 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make our lives easier, let's save this value as \"alphaI\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphaI <- 0.05 / 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now let's estimate a new 95% confidence ineterval for $\\mu_{\\text{horsebean}} - \\mu_{\\text{casein}}$ using the Bonferroni procedure. We will use the same formula as before with the Fisher's LSD method, however, we will need to change the value of $\\alpha$ when we are finding the quantile of the $t$ distribution that we need: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(160.2 - 326.8) - qt(alphaI / 2, df = 60 - 6, lower.tail = FALSE) * sqrt(3109 * (1/10 + 1/10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(160.2 - 326.8) + qt(alphaI / 2, df = 60 - 6, lower.tail = FALSE) * sqrt(3109 * (1/10 + 1/10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's conduct the hypothesis test again of $H_0: \\mu_{\\text{horsebean}} - \\mu_{\\text{casein}}=0$ versus $H_a: \\mu_{\\text{horsebean}} - \\mu_{\\text{casein}}\\neq 0$. We will have the *same* test statistic as before, however we will need a new critical value. \n",
    "\n",
    "**Question for you.** What is this new critical value considering our new $\\alpha_i$? Please answer below in a markdown cell. You can use software to find this critical value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we are conducting statistical hypothesis tests, we have two equivalent ways to determine our conclusion. The first is to see if the test statistic lies in the rejection region; the second is to directly compare the p-value for the test to the specified $\\alpha$ value. When we use the Bonferroni procedure, we also have two equivalent ways to determine our conclusion. The first is to determine the rejection region that would correspond to our lower $\\alpha_i$ value. The second would be to compare $m \\times \\text{p-value}$ with $\\alpha_E$. For this way, we compute the p-value as we would for the Fisher's LSD method, but we then multiply it by the total number of comparisons we will make and compare this adjusted p-value to the overall experimentwise Type I error probability, $\\alpha_E$. We call this new p-value a Bonferroni corrected or Bonferroni adjusted p-value.\n",
    "\n",
    "To do this for our test of $H_0: \\mu_{\\text{horsebean}} - \\mu_{\\text{casein}}=0$, we can take the p-value we computed before and multiply it by the number of comparisons we are going to make (15): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "15 * 2 * pt((160.2 - 326.8) / sqrt(3109 * (1/10 + 1/10)), df = 60 - 6, lower.tail = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question for you**. Determine the Bonferroni corrected p-value for the test of $H_0: \\mu_{\\text{soybean}} - \\mu_{\\text{linseed}} =0 $ versus $H_a: \\mu_{\\text{soybean}} - \\mu_{\\text{linseed}}\\neq 0$, considering that we will make all possible pairwise comparisons. Please insert a new code cell and determine this corrected p-value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now to use software to determine Bonferroni corrected p-values and confidence intervals we can use the same \"PostHocTest\" function as before, however we will change the method from \"lsd\" to \"bonf\" for Bonferroni: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PostHocTest(anova1, method = \"bonf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.3) The Tukey Honestly Significant Difference (HSD) procedure\n",
    "The Bonferroni procedure is known to be \"too conservative\". This means that if you specify an experimentwise Type I error probability $\\alpha_E$, and then you use the Bonferonni procedure, the actual $\\alpha_E$ that you acheive is lower than what you specify. So it solves the problem of the actual $\\alpha_E$ being greater than the specified $\\alpha_E$, by creating the opposite problem.\n",
    "\n",
    "**Question for you** Why would \"the actual $\\alpha_E$ that you acheive being lower than the $\\alpha_E$ you specify\" be a problem? Please answer this question in a new markdown cell below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Tukey HSD procedure is somewhat of a comprimise choice for when you want to make pairwise comparisons. If the assumptions we make for the overall ANOVA $F$-test are met (normally distributed populations, homogeneity of variances, and independence of each sample), then the Tukey HSD procedure can maintain the experimentwise $\\alpha_E$ at or below a specified $\\alpha_E$, *and*, the procedure will be less conservative than the Bonferroni approach. We don't discuss how to use the HSD procedure by hand, however it is a good procedure (often the best for one-way ANOVA) for you to be able to employ. To make Tukey HSD confidence intervals and adjusted p-values, we can use the \"TukeyHSD\" function. This function will take our ANOVA object that we created before as it's argument: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TukeyHSD(anova1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question for you**. In comparing the confidence intervals from the three methods we have discussed, Fisher's LSD, Bonferroni, and Tukey HSD, which are systematically the most narrow, and which are systematically the most wide. Please answer this question in a new markdown cell below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question for you**. If you wanted to preserve the experimentwise Type I error probability at 0.05 in comparing the chicken weights by feed type, what procedure would you use and why? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) A real analysis: Comparing urinary arsenic between different racial groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arsenic is a heavy metal that is toxic to humans. We are exposed to different heavy metals (including arsenic) in the food that we eat and the water that we drink. Some researchers are interested in whether arsenic exposure differs between racial groups, as there may be different dietary patterns within these groups. For examle, some racial groups may *on average* or typically consume more rice than others (a possible route of having small exposures to arsenic) or more seafood (another possible route of exposure depending on the type of fish and how it was raised). For this next example, let's assume that we are interested in determining whether there is evidence that the population means of urinary arsenic levels differ by ethnicity.\n",
    "\n",
    "I took a very small subset of the data from the National Health and Nutrition Examination Survey (https://www.cdc.gov/nchs/nhanes/index.htm). This study is \"a program of studies designed to assess the health and nutritional status of adults and children in the United States\" that has been running since the 1960's. Some of the de-identified datasets are publically availble. For our example we will look at a subset that I made for studying the relationship between urinary arsenic levels and ethnicity. **Caution** NHANES is designed to be used for making national level estimates and hypothesis tests that are valid at the nationwide level. However, to make this nationally representative you have to use a special system of weighting each observation differently. We will not do this for this example *hence* the inferences we make will not generalize to the population of the US. This subset is from the 2015-2016 cycle of this study. The data contains measurements of arsenic that were made from the participant's urine as well as demographic information about the included participants.\n",
    "\n",
    "First, we can retrieve the data from my NMSU site. Please run the following block of code below to retrieve and load the dataset as a \"data.frame\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataURL <- \"https://wordpress.nmsu.edu/ptrainor/files/2019/10/NHANES_Sample.xlsx\"\n",
    "httr::GET(dataURL, httr::write_disk(tf <- tempfile(fileext = \".xlsx\")))\n",
    "df1 <- as.data.frame(readxl::read_excel(tf, na = 'NA'))\n",
    "if(nrow(df1) > 0) print(\"The NHANES data is loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1) Some exploratory analyses\n",
    "The NHANES study has weird variable names. For us, we are looking for a variable called \"urxuas\", which is \"urinary total arsenic\" measured in Âµg/L. Let's first make a histogram showing the distribution of the \"urinary total arsenic\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(df1, aes(x = urxuas)) + \n",
    "   geom_histogram(bins = 50, color = \"black\", fill = \"grey80\") + \n",
    "   theme_bw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's make a qqplot of the same data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qqnorm(df1$urxuas)\n",
    "qqline(df1$urxuas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question for you** Given the histogram and Q-Q plot that you have just made of the total urinary arsenic in these human subjects, how would you describe the distribution of the data? Please comment on it's relationship to a normal distribution, if there is skew present (and if so in which direction)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2) A variable transformation\n",
    "We will discuss this further later in the course, but occasionally when we have data that does not appear to follow a normal distribution, we can utilize a variable transformation to see if the transformed data would be closer in distribution to a normal distribution. In other words, rather than conduct the ANOVA using the original measurements, we might try to use the transformed measurements after taking the natural log of the measurements or after taking the square root. For this example, let's try taking the natural log (in R this is just \"log\"). We will create a new variable and add it to the \"data.frame\" called \"LogUrinaryAs\" that is the log-transformed values of the original data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1$LogUrinaryAs <- log(df1$urxuas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question for you.** Using the codes from above for making a histogram and Q-Q plot (and copying, pasting, and modifying them), please make a histogram and Q-Q plot for the log-transformed values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question for you.** Do the transformed values appear to be closer in distribution to a normal distribution than the values on the original measurement scale?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now recall that our research question is to see if the levels of urinary arsenic differ between ethnicities. We will want to do an ANOVA $F$-test using the log-transformed data and we will want to make pairwise comparisons between the ethnicities. \n",
    "\n",
    "**Caution**, when we conduct these statistical tests, they will be for determining differences in the population means on the log-scale data, rather than on the original scale. This is often more appropriate when we have data that appears closer to being approximately normally distributed on the log-scale.\n",
    "\n",
    "The variable in the NHANES data that records the ethnicity of each subject is called \"ridreth3\". Let's take a look at what this variable looks like. We can use the \"head\" function to print out the first few observations of this variable. Remember that we use the \"\\\\$\" notation to retrieve a variable from a \"data.frame\". For example, \"df1$ridreth3\" will retrieve the \"ridreth3\" variable from the \"data.frame\" named \"df1\" for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(df1$ridreth3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so you will notice that the variable that tells us the ethnicity of each of the study subjects is represented as integers. This is very common in large datasets. Ethnicity is a nominal variable that has levels such as \"Non-Hispanic Asian\". Each integer in the data needs to be mapped to the appropriate level. We can find the documentation on this mapping (and other demographics) here: https://wwwn.cdc.gov/Nchs/Nhanes/2015-2016/DEMO_I.htm. For us, we will make a new variable called \"Ethnicity\" that is a copy of the \"ridreth3\" variable and then we will map these numbers to the different levels. Fist we will create this new variable as a copy of the other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1$Ethnicity <- df1$ridreth3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will convert this variable from a list of integer to a \"factor\" variable. This is R's way of saying a nominal variable. The \"as.factor\" variable will convert this variable from a set of integers to a nominal variable with different levels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1$Ethnicity <- as.factor(df1$Ethnicity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what these different levels are. The \"levels\" function will return for us what the different levels are that can be found in the nominal variable \"Ethnicity\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels(df1$Ethnicity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to map these levels to the levels that we find in the NHANES documentation. We will do this by telling R that \"1\" should be replaced by \"Mexican American\", \"2\" should be replaced by \"Other Hispanic\" and so on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels(df1$Ethnicity) <- c(\"Mexican American\", \"Other Hispanic\", \"Non-Hispanic White\",\n",
    "                          \"Non-Hispanic Black\", \"Non-Hispanic Asian\", \n",
    "                           \"Other Race - Including Multi-Racial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now let's look at the first 6 observations of this \"Ethnicity\" variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(df1$Ethnicity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for our own interest, let's count how many subjects of each ethnicity we have. We can do this using the table function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table(df1$Ethnicity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now let's make a boxplot of the log-transformed urinary arsenic data colored by each ethnicity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width = 7, repr.plot.height = 4)\n",
    "ggplot(df1, aes(x = Ethnicity, y = LogUrinaryAs, fill = Ethnicity)) + geom_boxplot() + \n",
    "   theme_bw() + theme(axis.text.x=element_blank())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also determine the sample means of the log-transformed urinary total arsenic by group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate(LogUrinaryAs ~ Ethnicity, data = df1, mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the ANVOA object so that we can see the ANOVA table and determine the result of our ANOVA overall $F$-test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anova2 <- aov(LogUrinaryAs ~ Ethnicity, data = df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"summary\" function will provide a summary table of the ANOVA for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(anova2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use Tukey's method to determine confidence intervals for the difference in population means:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TukeyHSD(anova2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question for you.** Which ethnicities appear to have different levels of urinary total arsenic from each other? Please answer in a new markdown cell below. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
