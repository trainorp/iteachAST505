{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0) Instructions:\n",
    "Please complete the workbook below. Some of the calculations are already ready to be \"run\". However, please read the text carefully to find questions that you should answer for credit. Remember that to answer a question in text, you click \"insert\", then \"insert cell below\", switch the input from \"code\" to \"markdown\", type your answer, and finally click the run button to set your text in stone. In a few questions, you will need to do some calculations on your own. Hint: for these calculations you can copy, paste, and modify code that is above the calculation that you need to do. You may work by yourself or in groups of 2. Please remember to put your name on top, remember to save the workbook, and remember to upload to Canvas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please read** Please run the code below to install the \"DescTools\", \"EnvStats\", and \"Stat2Data\" packages. **Do not proceed until the star goes away! Do not click run twice**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "install.packages('DescTools', dep = TRUE)\n",
    "install.packages('EnvStats', dep = TRUE)\n",
    "install.packages('Stat2Data', dep = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please read** Please run the code below and ignore the output. This code is simply setting the height and width of the plots we will make later on and loading the packages that we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Attaching package: 'EnvStats'\n",
      "\n",
      "The following objects are masked from 'package:stats':\n",
      "\n",
      "    predict, predict.lm\n",
      "\n",
      "The following object is masked from 'package:base':\n",
      "\n",
      "    print.default\n",
      "\n",
      "Registered S3 methods overwritten by 'ggplot2':\n",
      "  method         from \n",
      "  [.quosures     rlang\n",
      "  c.quosures     rlang\n",
      "  print.quosures rlang\n",
      "Registered S3 method overwritten by 'rvest':\n",
      "  method            from\n",
      "  read_xml.response xml2\n",
      "-- Attaching packages --------------------------------------- tidyverse 1.2.1 --\n",
      "v ggplot2 3.1.1       v purrr   0.3.2  \n",
      "v tibble  2.1.1       v dplyr   0.8.0.1\n",
      "v tidyr   0.8.3       v stringr 1.4.0  \n",
      "v readr   1.3.1       v forcats 0.4.0  \n",
      "-- Conflicts ------------------------------------------ tidyverse_conflicts() --\n",
      "x dplyr::filter() masks stats::filter()\n",
      "x dplyr::lag()    masks stats::lag()\n"
     ]
    }
   ],
   "source": [
    "library(DescTools)\n",
    "library(EnvStats)\n",
    "library(tidyverse)\n",
    "library(Stat2Data)\n",
    "options(repr.plot.width = 6, repr.plot.height = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Comparing two population variances\n",
    "To start off we are going to go through the process of comparing two population variances. We will use a very strange real dataset from a study looking at the pH of the brains of humans who have recently died. We will only use a subset of this data but the citation is here: \"Jun Z. Li et al. (2013), \"Circadian patterns of gene expression in the human brain and disruption in major depressive disorder,\" PNAS, vol 110, no. 24, www.pnas.org/cgi/doi/10.1073/pnas.1305814110\"\n",
    "\n",
    "The first thing that we will do is load the data. Because this data in one of the packages that we loaded (\"Stat2Data\"), if we use the \"data\" command with the name of the dataset, \"BrainpH\", it will load this data and make it available to us as \"BrainpH\". When running this command you shouldn't see any output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data(BrainpH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how many observations are in this dataset using the \"nrow\" function which counts the number of rows of a \"data.frame\" or dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrow(BrainpH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since that is a good amount of observations, let's just print the first 6 observations to see what this dataset looks like. To do this we will use the \"head\" function which prints out the first 6 rows of a \"data.frame\": "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(BrainpH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1) Hypothesis test involving two population variances by hand:\n",
    "Okay, let's say you want to know if the pH of brains from recently deceased humans has higher variability in females than in males. Let's call females \"Population 1\" and males \"Population 2\". \n",
    "\n",
    "**Question for you** To determine if there is evidence that the population variance of pH measurements is higher in females than in males, what would your null and alternative hypotheses be?. Please answer this question in a new markdown cell below. You do not need to use symbols, you can say things like (for example): \"sigma squred from population 1 is less than or equal to...\" rather than $\\sigma_1^2 \\leq ...$\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now that we have specified our hypotheses, let's conduct a formal statistical test with $\\alpha = 0.10$. To do this we will need a test statistic $F^* = s_1^2 / s_2^2$. So we will need to find the sample variances. For this we can use the \"aggregate\" function. This function applies a function to a variable *by a grouping variable*. So for us, we want to compute the sample variance of pH measurements separately by sex (so sex is our grouping variable). The aggregate function has a few arguments, a formula (in our case \"pH ~ Sex\") that means apply a function to \"pH\" separately by \"Sex\", \"data\" which specifies what dataset the \"pH\" and \"Sex\" variables are from, and \"FUN\" which specifies which function we would like to apply. For us we want the sample variance so we will use the \"var\" fun. Please run the code below to calculate the sample variance of pH measurements by sex: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate(pH ~ Sex, data = BrainpH, FUN = var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question for you.** Now that you have computed the sample means, what is the value of the test statistic, $F^*$? Please compute it using code below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, we now have our set of hypotheses, we know $\\alpha = 0.10$, and we have a test statistic. Let's now find the rejection region / rejection rule for determining if we have evidence to reject the null hypothesis. For this we will need quantiles from the $F$ distribution. To find these, we need to know $\\alpha$, $\\text{df}_1$, and $\\text{df}_2$. To find $\\text{df}_1$, and $\\text{df}_2$, we need to know the sample size from each populations (female and male). We can use the \"aggregate\" function again. It will have the same arguments, except instead of using \"FUN = var\" to determine the sample variance of pH measurements by sex, we will use \"FUN = length\". This will count the number of pH measurements by sex, which will give us the sample sizes we need. Please run this code below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate(pH ~ Sex, data = BrainpH, length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to determine the critical value from the $F$ distribution with $\\text{df}_1 = 10-1$, and $\\text{df}_2 = 44-1$. We will use the \"qf\" function which stands for \"quantile from an $F$ distribution\". We will use the \"lower.tail = FALSE\" function to look for the quantile that gives us a right tailed probability for $\\alpha$: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qf(0.10, df1 = 10 - 1, df2 = 44 - 1, lower.tail = FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question for you.** Please state the rejection rule for this test in a new markdown cell below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question for you.** Now that you have a set of hypotheses, a rejection rule, and a test statistic, what is the conclusion of this test. Please answer below in a new markdown cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to compute a p-value or the level of significance for this test, we will look for $P(F>F^*)$. To do this we can use the \"pf\" function (\"probability from an $F$ distribution\"), with the correct degrees of freedom and using \"lower.tail = FALSE\" to get the right-tailed probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf(0.09311667 / 0.04730883, df1 = 10 - 1, df2 = 44 - 1, lower.tail = FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2) Hypothesis test involving two population variances by hand:\n",
    "As you might expect, we can do all of the work we did in 1.1 using just one function in R \"var.test\". To use this function we will want to specify an \"x\" corresponding to the first population and a \"y\" corresponding to the second population. We will also have to specify what kind of alternative hypothesis we are posing. In this case, $H_a: \\sigma_1^2 > \\sigma_2^2$, which means we will specify 'alternative = \"greater\"'. There is one other thing to note: when we specify our measurements \"x\" and \"y\", we need them to a be a simple list of numbers. So we will use \"[]\" notation to subset our data by Sex. To see how it works, please run the following two lines before running the code with \"var.test\": "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BrainpH$pH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BrainpH$pH[BrainpH$Sex == \"F\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var.test(x = BrainpH$pH[BrainpH$Sex == \"F\"], y = BrainpH$pH[BrainpH$Sex == \"M\"], alternative = \"greater\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3) A confidence interval for the ratio of population variances \"by hand\": \n",
    "Now, let's determine a confidence interval for the ratio of the population variances (of brain pH) between females and males. We will continue to have females be Population 1 and males to be Population 2. Let's determine a 90% confidence interval for this ratio. Our first step is going to be to determine $F_U = F_{\\alpha / 2, \\text{df}_2, \\text{df}_1}$. This one will be easy. We will use the \"qf\" function to look for the quantile that gives us a right-tailed probability of $\\alpha / 2$. To make sure we get the quantile for the right-tailed probability we will use \"lower.tail = FALSE\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qf(0.10 / 2, 44 - 1, 10 - 1, lower.tail = FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now to find $F_L$ is a bit more difficult. In the lecture we discussed that to use the textbook appendix tables, we will have to use the following equation: $F_L = F_{1-\\alpha / 2, \\text{df}_2, \\text{df}_1} = 1 / F_{\\alpha / 2, \\text{df}_1, \\text{df}_2}$. So here we will find $1 / F_{\\alpha / 2, \\text{df}_1, \\text{df}_2}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 / qf(0.10 / 2, 10 - 1, 44 - 1, lower.tail = FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, unlike the textbook appendix tables, we can find any value from the $F$ distribution using R, so we could have asked for $F_{1-\\alpha / 2, \\text{df}_2, \\text{df}_1}$ directly: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qf(1 - 0.10 / 2, 44 - 1, 10 - 1, lower.tail = FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question for you.** What are the values of $F_L$ and $F_U$. Please answer in a new markdown cell below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to determine the 90% confidence interval for the ratio of population variances. We will use the formula $\\left(\\frac{s_1^2}{s_2^2} F_L, \\frac{s_1^2}{s_2^2}F_U \\right)$ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.09311667 / 0.04730883 * qf(1 - 0.10 / 2, 44 - 1, 10 - 1, lower.tail = FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.09311667 / 0.04730883 * qf(0.10 / 2, 44 - 1, 10 - 1, lower.tail = FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question for you**. What is the 90% confidence interval for the ratio of population variances. Please answer in a new markdown cell below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4) A CI for the ratio of population variances using software\n",
    "Now we can use the \"var.test\" function again for determining a confidence interval. We will specify 'alternative = \"two.sided\"' so that we get a confidence interval that has symmetric Type I error probabilities on the left and right hand side (like all the confidence intervals we make in this class. Since we want a 90% confidence interval we will specify \"conf.level = 0.90\": "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var.test(BrainpH$pH[BrainpH$Sex == \"F\"], BrainpH$pH[BrainpH$Sex == \"M\"], \n",
    "         alternative = \"two.sided\", conf.level = 0.90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question for you.** Now, please determine an 80% confidence interval for the ratio of population variances by copying, pasting, and modifying the code from above in a new code cell below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question for you.** What value of the ratio of population variances would correspond to the variances being equal between the two groups? Is this value contained in the 90% confidence interval? Is this value contained in the 80% confidence interval? Which one of these intervals agrees with the result of the hypothesis test? Please answer these questions in a new markdown cell below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Analysis of Variance Part 1: The overall $F$ test\n",
    "Now in this part of the lab we will change directions and dicuss comparing means between three or more populations. As an example we will analyze the weights of chickens that were fed various types of feed. This data was published in the journal *Biometrika* in 1948. Below, we will load the data using the \"data\" command, and we will also do some sampling from this dataset (this involves the \"group_by\" and \"sample_n\") commands. Feel free to run and ignore this part. Basically, the original data has unbalanced sample sizes (some $n$'s are more than others) and we have not yet discussed how to deal with this case. So we will take 10 observations from each population:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data(chickwts)\n",
    "set.seed(3)\n",
    "chickwts <- chickwts %>% group_by(feed) %>% sample_n(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now that the data is available to us, let's use the \"head\" function to take a look at the first 6 observations: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(chickwts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From that you can observe that one of the types of feeds for these chickens was \"horsebean\". Let's take a look at the other types of feed that were included in this experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table(chickwts$feed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we discuss conducting an Analysis of Variance, let's take a look at the weight data from one of the populations of chickens (that was fed linseed)\". Let's determine if these weights appear to be approximately normal. Please run the code below to make a Q-Q plot of the data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qqnorm(chickwts$weight[chickwts$feed == \"linseed\"])\n",
    "qqline(chickwts$weight[chickwts$feed == \"linseed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question for you.** Based on the above Q-Q plot, would you say that the data is approximately normally distributed? Please answer below in a markdown cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question for you**. Imagine that you want to determine if there is evidence that the chicken weights depends on the type of feed that is given to the chickens. Below, please explain what your null and alternative hypotheses would be for which we will use an overall $F$ test. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we formally conduct an ANOVA $F$ test, let's take a look at the distribution of the weights for each sample with a different type of feed. Below we will make a boxplot that has feed on the horizontal axis (x), weight on the vertical axis (y), and is also collored (fill) inside by the feed type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(chickwts, aes(x = feed, fill = feed, y = weight)) + geom_boxplot() + theme_bw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our ANOVA $F$ test involves population means we might want to add the means within each sample to the boxplot. Below, we will add the means as red diamonds. Also in this version, we will add the \"alpha = .5\" to the geom_boxplot layer to make the color more translucent. Art!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(chickwts, aes(x = feed, fill = feed, y = weight)) + geom_boxplot(alpha = .5) + \n",
    "  stat_summary(fun.y = mean, colour = \"darkred\", geom = \"point\", shape = 18, size = 3, show.legend = FALSE) +\n",
    "  theme_bw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question for you.** Based on the boxplot with the means shown above, do you think we will find evidence that the mean weight of chickens does depend on the type of feed? Please answer below in a new markdown cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1) Conducting an ANOVA $F$ test \"by hand\"\n",
    "Let's assume that we want to conduct an ANOVA to determine if we have sufficient evidence $\\alpha = 0.01$, that the mean chicken weights differ by feed type. We have a long process to conduct an ANOVA $F$ test \"by hand\". Once we have specified our hypotheses (you have already done this in section #2), we will need to determine a critical value from an $F$ distribution that will give us a rejection rule, and we will need to determine our $F$ test statistic.  Let's start by determining the $F$ test statistic. For this we will need to determine the within-sample sum of squares (SSW) and the sum of squares between samples (SSB). Let's start with SSB first. \n",
    "\n",
    "Remember that for SSB, we use the following: $\\sum_{j = 1}^k n_j (\\bar{y}_{j.} - \\bar{y}_{..} )^2$\n",
    "\n",
    "So we will need to calculate $\\bar{y}_{j.}$ which is the mean weight for each type of feed $j$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate(weight ~ feed, data = chickwts, mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we can see the means from each group. Now, let's save this set of means as a new \"data.frame\" called \"groupMeans\": "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupMeans <- aggregate(weight ~ feed, data = chickwts, mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to determine $\\bar{y}_{..}$, we will need to take the mean of the $\\bar{y}_{j.}$'s: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(groupMeans$weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to calculate the $\\bar{y}_{j.} - \\bar{y}_{..} $'s we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupMeans$weight - mean(groupMeans$weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so now let's save these $\\bar{y}_{j.} - \\bar{y}_{..}$ values as a new column in our \"data.frame\" that had the group means. This will allows us to square them and sum them up later on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupMeans$meanMinusGrandMean <- groupMeans$weight - mean(groupMeans$weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, let's pause here and see what this \"data.frame\" looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now let's add a new column that will have the $(\\bar{y}_{j.} - \\bar{y}_{..} )^2$ values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupMeans$meanMinusGrandMeanSq <- groupMeans$meanMinusGrandMean^2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's take a look at this \"data.frame\" now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have everything that we need to compute $\\text{SSB} = \\sum_{j = 1}^k n_j (\\bar{y}_{j.} - \\bar{y}_{..} )^2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(10 * groupMeans$meanMinusGrandMeanSq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(10 * groupMeans$meanMinusGrandMeanSq) / (6-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have SSB, let's calculate SSW. To do this we will use the formula: $(n_1 - 1) s_1^2 + (n_2 - 1) s_2^2 + (n_3 - 1) s_3^2 + ... + (n_k - 1) s_k^2$. We can use the \"aggregate\" function as we did before to calculate the sample variance for each group: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate(weight ~ feed, data = chickwts, FUN = var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, let's save the above \"data.frame\" that has all of the sample variances (of chicken weights) by feed as \"varByGroup\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varByGroup <- aggregate(weight ~ feed, data = chickwts, FUN = var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, no to find SSW, we need to multiply each one of these sample variances by the relevant sample size minus 1 (here this is 9 for all groups) and add them all together. We do this below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(9 * varByGroup$weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question for you.** What are the values of SSB and SSW? Please answer below in a new markdown cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next step prior to determining our ANOVA $F$ statistic will be to determine the mean squares, $s_B^2$ and $s_W^2$, by using $s_B^2 = \\frac{\\text{SSB}}{k-1}$ and $s_W^2 = \\frac{SSW}{N-k}$. Below, in separate cells we will calculate each. We will then save them as \"s2b\" and \"s2w\". Please run the next three cells to do this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "209065.95 / (6 - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "170071.7 / (60 - 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2b <- 209065.95 / (6 - 1)\n",
    "s2w <- 170071.7 / (60 - 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now that we have these mean squares we can determine our ANOVA $F$-test statistic: $F^* = s^2_B / s^2_W$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2b / s2w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to determine a critical value, to see if we have evidence that supports rejecting $H_0$. We are going to look for a quantile from the $F$ distribution that has $k-1$ and $N-k$ degrees of freedom. You will notice that these degrees of freedom are the same integers that we divided SSB and SSW by in order to find the mean squares. As an example of how to do this, let's find the quantile from an $F$ distribution with 4 and 40 degrees of freedom that would give us a quantile for a right-tailed probability of 0.025 (please note this is not the quantile we need for our ANOVA):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qf(.025, df1 = 4, df2 = 40, lower.tail = FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question for you.** Determine the correct critical value from an $F$ distribution that we need for conducting our overall ANOVA $F$-test. You should copy, paste, and modify the code from above to find this critical value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question for you**. Based on your observed test statistic and the critical value corresponding to this test setup, what is your conclusion? In terms of the original problem involving feed type and chicken weights, what is your conclusion? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will determine the level of significance of the test that we have conducted. To do this we will need to determine $P(F \\geq F^*)$. As an example calculation, below you will find $P(F \\geq 10)$ from an $F$ distribution with 4 and 40 degrees of freedom: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf(10, 4, 40, lower.tail = FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question for you.** Copy, paste, and modify the code from above in a new code cell below to determine the level of significance of the overall ANOVA $F$-test that we have conducted. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2) Conducting an ANOVA $F$-test using software"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conducting an ANOVA $F$-test using R is super easy and fun. We will use the \"aov\" function. The first argument to this function is called the formula. This has the following form: \"Response variable ~ Group variable\". So for us, this will be \"weight ~ feed\" to examine if there are differences in the population means of chicken weights by feed type. the next argument is \"data\" which tells R which \"data.frame\" contains these two variables. We wrap the \"aov\" function in another function called \"summary\". What this function does is provide us with a nice summary table from the ANOVA $F$-test, showing us SSB, SSW, the mean squares, the $F^*$ test statistic, and the p-value for the test. Please run the code to see this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "            Df Sum Sq Mean Sq F value   Pr(>F)    \n",
       "feed         5 209066   41813   13.28 1.92e-08 ***\n",
       "Residuals   54 170072    3149                     \n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary(aov(weight ~ feed, data = chickwts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>weight</th><th scope=col>feed</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>404      </td><td>casein   </td></tr>\n",
       "\t<tr><td>222      </td><td>casein   </td></tr>\n",
       "\t<tr><td>352      </td><td>casein   </td></tr>\n",
       "\t<tr><td>260      </td><td>casein   </td></tr>\n",
       "\t<tr><td>390      </td><td>casein   </td></tr>\n",
       "\t<tr><td>379      </td><td>casein   </td></tr>\n",
       "\t<tr><td>216      </td><td>casein   </td></tr>\n",
       "\t<tr><td>359      </td><td>casein   </td></tr>\n",
       "\t<tr><td>283      </td><td>casein   </td></tr>\n",
       "\t<tr><td>368      </td><td>casein   </td></tr>\n",
       "\t<tr><td>160      </td><td>horsebean</td></tr>\n",
       "\t<tr><td>217      </td><td>horsebean</td></tr>\n",
       "\t<tr><td>124      </td><td>horsebean</td></tr>\n",
       "\t<tr><td>143      </td><td>horsebean</td></tr>\n",
       "\t<tr><td>179      </td><td>horsebean</td></tr>\n",
       "\t<tr><td>168      </td><td>horsebean</td></tr>\n",
       "\t<tr><td>227      </td><td>horsebean</td></tr>\n",
       "\t<tr><td>140      </td><td>horsebean</td></tr>\n",
       "\t<tr><td>136      </td><td>horsebean</td></tr>\n",
       "\t<tr><td>108      </td><td>horsebean</td></tr>\n",
       "\t<tr><td>213      </td><td>linseed  </td></tr>\n",
       "\t<tr><td>169      </td><td>linseed  </td></tr>\n",
       "\t<tr><td>203      </td><td>linseed  </td></tr>\n",
       "\t<tr><td>181      </td><td>linseed  </td></tr>\n",
       "\t<tr><td>257      </td><td>linseed  </td></tr>\n",
       "\t<tr><td>148      </td><td>linseed  </td></tr>\n",
       "\t<tr><td>229      </td><td>linseed  </td></tr>\n",
       "\t<tr><td>271      </td><td>linseed  </td></tr>\n",
       "\t<tr><td>309      </td><td>linseed  </td></tr>\n",
       "\t<tr><td>260      </td><td>linseed  </td></tr>\n",
       "\t<tr><td>303      </td><td>meatmeal </td></tr>\n",
       "\t<tr><td>380      </td><td>meatmeal </td></tr>\n",
       "\t<tr><td>263      </td><td>meatmeal </td></tr>\n",
       "\t<tr><td>153      </td><td>meatmeal </td></tr>\n",
       "\t<tr><td>257      </td><td>meatmeal </td></tr>\n",
       "\t<tr><td>315      </td><td>meatmeal </td></tr>\n",
       "\t<tr><td>325      </td><td>meatmeal </td></tr>\n",
       "\t<tr><td>258      </td><td>meatmeal </td></tr>\n",
       "\t<tr><td>206      </td><td>meatmeal </td></tr>\n",
       "\t<tr><td>242      </td><td>meatmeal </td></tr>\n",
       "\t<tr><td>316      </td><td>soybean  </td></tr>\n",
       "\t<tr><td>267      </td><td>soybean  </td></tr>\n",
       "\t<tr><td>193      </td><td>soybean  </td></tr>\n",
       "\t<tr><td>158      </td><td>soybean  </td></tr>\n",
       "\t<tr><td>243      </td><td>soybean  </td></tr>\n",
       "\t<tr><td>248      </td><td>soybean  </td></tr>\n",
       "\t<tr><td>199      </td><td>soybean  </td></tr>\n",
       "\t<tr><td>271      </td><td>soybean  </td></tr>\n",
       "\t<tr><td>329      </td><td>soybean  </td></tr>\n",
       "\t<tr><td>230      </td><td>soybean  </td></tr>\n",
       "\t<tr><td>226      </td><td>sunflower</td></tr>\n",
       "\t<tr><td>295      </td><td>sunflower</td></tr>\n",
       "\t<tr><td>322      </td><td>sunflower</td></tr>\n",
       "\t<tr><td>339      </td><td>sunflower</td></tr>\n",
       "\t<tr><td>318      </td><td>sunflower</td></tr>\n",
       "\t<tr><td>423      </td><td>sunflower</td></tr>\n",
       "\t<tr><td>392      </td><td>sunflower</td></tr>\n",
       "\t<tr><td>334      </td><td>sunflower</td></tr>\n",
       "\t<tr><td>341      </td><td>sunflower</td></tr>\n",
       "\t<tr><td>340      </td><td>sunflower</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       " weight & feed\\\\\n",
       "\\hline\n",
       "\t 404       & casein   \\\\\n",
       "\t 222       & casein   \\\\\n",
       "\t 352       & casein   \\\\\n",
       "\t 260       & casein   \\\\\n",
       "\t 390       & casein   \\\\\n",
       "\t 379       & casein   \\\\\n",
       "\t 216       & casein   \\\\\n",
       "\t 359       & casein   \\\\\n",
       "\t 283       & casein   \\\\\n",
       "\t 368       & casein   \\\\\n",
       "\t 160       & horsebean\\\\\n",
       "\t 217       & horsebean\\\\\n",
       "\t 124       & horsebean\\\\\n",
       "\t 143       & horsebean\\\\\n",
       "\t 179       & horsebean\\\\\n",
       "\t 168       & horsebean\\\\\n",
       "\t 227       & horsebean\\\\\n",
       "\t 140       & horsebean\\\\\n",
       "\t 136       & horsebean\\\\\n",
       "\t 108       & horsebean\\\\\n",
       "\t 213       & linseed  \\\\\n",
       "\t 169       & linseed  \\\\\n",
       "\t 203       & linseed  \\\\\n",
       "\t 181       & linseed  \\\\\n",
       "\t 257       & linseed  \\\\\n",
       "\t 148       & linseed  \\\\\n",
       "\t 229       & linseed  \\\\\n",
       "\t 271       & linseed  \\\\\n",
       "\t 309       & linseed  \\\\\n",
       "\t 260       & linseed  \\\\\n",
       "\t 303       & meatmeal \\\\\n",
       "\t 380       & meatmeal \\\\\n",
       "\t 263       & meatmeal \\\\\n",
       "\t 153       & meatmeal \\\\\n",
       "\t 257       & meatmeal \\\\\n",
       "\t 315       & meatmeal \\\\\n",
       "\t 325       & meatmeal \\\\\n",
       "\t 258       & meatmeal \\\\\n",
       "\t 206       & meatmeal \\\\\n",
       "\t 242       & meatmeal \\\\\n",
       "\t 316       & soybean  \\\\\n",
       "\t 267       & soybean  \\\\\n",
       "\t 193       & soybean  \\\\\n",
       "\t 158       & soybean  \\\\\n",
       "\t 243       & soybean  \\\\\n",
       "\t 248       & soybean  \\\\\n",
       "\t 199       & soybean  \\\\\n",
       "\t 271       & soybean  \\\\\n",
       "\t 329       & soybean  \\\\\n",
       "\t 230       & soybean  \\\\\n",
       "\t 226       & sunflower\\\\\n",
       "\t 295       & sunflower\\\\\n",
       "\t 322       & sunflower\\\\\n",
       "\t 339       & sunflower\\\\\n",
       "\t 318       & sunflower\\\\\n",
       "\t 423       & sunflower\\\\\n",
       "\t 392       & sunflower\\\\\n",
       "\t 334       & sunflower\\\\\n",
       "\t 341       & sunflower\\\\\n",
       "\t 340       & sunflower\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| weight | feed |\n",
       "|---|---|\n",
       "| 404       | casein    |\n",
       "| 222       | casein    |\n",
       "| 352       | casein    |\n",
       "| 260       | casein    |\n",
       "| 390       | casein    |\n",
       "| 379       | casein    |\n",
       "| 216       | casein    |\n",
       "| 359       | casein    |\n",
       "| 283       | casein    |\n",
       "| 368       | casein    |\n",
       "| 160       | horsebean |\n",
       "| 217       | horsebean |\n",
       "| 124       | horsebean |\n",
       "| 143       | horsebean |\n",
       "| 179       | horsebean |\n",
       "| 168       | horsebean |\n",
       "| 227       | horsebean |\n",
       "| 140       | horsebean |\n",
       "| 136       | horsebean |\n",
       "| 108       | horsebean |\n",
       "| 213       | linseed   |\n",
       "| 169       | linseed   |\n",
       "| 203       | linseed   |\n",
       "| 181       | linseed   |\n",
       "| 257       | linseed   |\n",
       "| 148       | linseed   |\n",
       "| 229       | linseed   |\n",
       "| 271       | linseed   |\n",
       "| 309       | linseed   |\n",
       "| 260       | linseed   |\n",
       "| 303       | meatmeal  |\n",
       "| 380       | meatmeal  |\n",
       "| 263       | meatmeal  |\n",
       "| 153       | meatmeal  |\n",
       "| 257       | meatmeal  |\n",
       "| 315       | meatmeal  |\n",
       "| 325       | meatmeal  |\n",
       "| 258       | meatmeal  |\n",
       "| 206       | meatmeal  |\n",
       "| 242       | meatmeal  |\n",
       "| 316       | soybean   |\n",
       "| 267       | soybean   |\n",
       "| 193       | soybean   |\n",
       "| 158       | soybean   |\n",
       "| 243       | soybean   |\n",
       "| 248       | soybean   |\n",
       "| 199       | soybean   |\n",
       "| 271       | soybean   |\n",
       "| 329       | soybean   |\n",
       "| 230       | soybean   |\n",
       "| 226       | sunflower |\n",
       "| 295       | sunflower |\n",
       "| 322       | sunflower |\n",
       "| 339       | sunflower |\n",
       "| 318       | sunflower |\n",
       "| 423       | sunflower |\n",
       "| 392       | sunflower |\n",
       "| 334       | sunflower |\n",
       "| 341       | sunflower |\n",
       "| 340       | sunflower |\n",
       "\n"
      ],
      "text/plain": [
       "   weight feed     \n",
       "1  404    casein   \n",
       "2  222    casein   \n",
       "3  352    casein   \n",
       "4  260    casein   \n",
       "5  390    casein   \n",
       "6  379    casein   \n",
       "7  216    casein   \n",
       "8  359    casein   \n",
       "9  283    casein   \n",
       "10 368    casein   \n",
       "11 160    horsebean\n",
       "12 217    horsebean\n",
       "13 124    horsebean\n",
       "14 143    horsebean\n",
       "15 179    horsebean\n",
       "16 168    horsebean\n",
       "17 227    horsebean\n",
       "18 140    horsebean\n",
       "19 136    horsebean\n",
       "20 108    horsebean\n",
       "21 213    linseed  \n",
       "22 169    linseed  \n",
       "23 203    linseed  \n",
       "24 181    linseed  \n",
       "25 257    linseed  \n",
       "26 148    linseed  \n",
       "27 229    linseed  \n",
       "28 271    linseed  \n",
       "29 309    linseed  \n",
       "30 260    linseed  \n",
       "31 303    meatmeal \n",
       "32 380    meatmeal \n",
       "33 263    meatmeal \n",
       "34 153    meatmeal \n",
       "35 257    meatmeal \n",
       "36 315    meatmeal \n",
       "37 325    meatmeal \n",
       "38 258    meatmeal \n",
       "39 206    meatmeal \n",
       "40 242    meatmeal \n",
       "41 316    soybean  \n",
       "42 267    soybean  \n",
       "43 193    soybean  \n",
       "44 158    soybean  \n",
       "45 243    soybean  \n",
       "46 248    soybean  \n",
       "47 199    soybean  \n",
       "48 271    soybean  \n",
       "49 329    soybean  \n",
       "50 230    soybean  \n",
       "51 226    sunflower\n",
       "52 295    sunflower\n",
       "53 322    sunflower\n",
       "54 339    sunflower\n",
       "55 318    sunflower\n",
       "56 423    sunflower\n",
       "57 392    sunflower\n",
       "58 334    sunflower\n",
       "59 341    sunflower\n",
       "60 340    sunflower"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chickwts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
