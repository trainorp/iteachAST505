\documentclass[xcolor=dvipsnames]{beamer} 
\usetheme{AnnArbor}
\usecolortheme{beaver}

\usepackage{amsmath,graphicx,booktabs,tikz,subfig,color,lmodern}
\definecolor{mycol}{rgb}{.4,.85,1}
\setbeamercolor{title}{bg=mycol,fg=black} 
\setbeamercolor{palette primary}{use=structure,fg=white,bg=red}
\setbeamercolor{block title}{fg=white,bg=red!50!black}
% \setbeamercolor{block title}{fg=white,bg=blue!75!black}

\title[Lecture 14]{Lecture 14: Analysis of Variance}
\author[Patrick Trainor]{Patrick Trainor, PhD, MS, MA}
\institute[NMSU]{New Mexico State University}
\date{October}

\begin{document}
	
\begin{frame}
	\maketitle
\end{frame}

\begin{frame}{Outline}
	\tableofcontents[hideallsubsections]
\end{frame}

\section{Introduction}

\begin{frame}{Outline}
	\tableofcontents[currentsection,subsectionstyle=show/shaded/hide]
\end{frame}

\begin{frame}{Introduction}
	\begin{center}
		\includegraphics[width=.9\linewidth]{Trees1}
	\end{center}
\end{frame}

\begin{frame}{Introduction}
	\begin{center}
		\includegraphics[width=.9\linewidth]{Trees2}
	\end{center}
\end{frame}

\begin{frame}{Introduction}
	\begin{itemize}
		\item \textbf{Within-group (sample) variation:} Variability of individual measurements that are all from one group (sample). 
		\begin{itemize}
			\item Not all of the measurements from one group will be equal to the mean in that group
			\item This type of variation may be related to the independent variable
		\end{itemize}
		\item[]
		\item \textbf{Between-group (sample) variation:} Variability between separate groups of interest. 
		\begin{itemize}
			\item Not all of the means for each group will be identical
			\item This type of variation may be related to the independent variable
		\end{itemize}
		\item[]
		\item When the between-group variation is large relative to the within-group variation we have evidence that population means are different 
	\end{itemize}
\end{frame}

\begin{frame}{Introduction}
	\begin{center}
		\includegraphics[width=.9\linewidth]{Trees1b}
	\end{center}
\end{frame}

\begin{frame}{Introduction}
	\begin{center}
		\includegraphics[width=.9\linewidth]{Trees2b}
	\end{center}
\end{frame}

\begin{frame}{Introduction}
	\begin{center}
		\includegraphics[width=1\linewidth]{Trees3}
	\end{center}
\end{frame}

\begin{frame}{Introduction}
	\begin{itemize}
		\item \textbf{Analysis of Variance:} ``All differences in sample means are judged statistically
		significant (or not) by comparing them to the variation within samples''
		\item[]
		\item ``Analysis of Variance'' (ANOVA) is not about comparing population variances between multiple groups
		\item[]
		\item In general if we have $p$ groups, we want to test the hypotheses:
		\begin{itemize}
			\item $H_0: \mu_1 = \mu_2 = \hdots = \mu_p$
			\item $H_a: $ At least one of the means, say $\mu_j$ ($j$ denotes a specific group), is not equal to the rest 
			\item[]
		\end{itemize}
		\item We cannot use pairwise $t$-tests for more than 2 groups because the true Type I error rates will be much greater than specified
	\end{itemize}
\end{frame}

\section{The ANOVA framework}

\begin{frame}{Outline}
	\tableofcontents[currentsection,subsectionstyle=show/shaded/hide]
\end{frame}

\begin{frame}{If $H_0$ were true}
	\begin{itemize}
		\item To develop a test of $H_a$ vs $H_0$, we need to determine a test statistic and determine the distribution of the test statistic if the null hypothesis were true. For this we will assume:
		\begin{enumerate}
			\item Each of the $p$ groups (populations) has a normal distribution
			\item[]
			\item The variances of the $p$ groups (populations) are equal
			\item[]
			\item The measurements for each group $j$ is an independent random samples from their respective populations
			\item[]
		\end{enumerate}
	\item Now we can measure variability using two measures:
	\begin{itemize}
		\item $s_W^2$: Within-group variance 
		\item[]
		\item $s_B^2$: Between-group variance 
	\end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}{If $H_0$ were true}
	\begin{itemize}
		\item $s_W^2$: Within-group variance 
		\begin{itemize}
			\item If the null hypothesis were true, then each group would have the same population variance about the same population mean
			\item[]
			\item So then we could estimate this population variance using each sample (from each different group) as:
			\begin{gather*}
				s_W^2 = \frac{\sum_{j=1}^p (n_j -1) s_j^2}{\sum_{j=1}^p (n_j - 1)}
			\end{gather*}
			\item Notice that we have seen this before in $t$-tests that assumed equal variances. In this case $p=2$, and:
			\begin{gather*}
			s_W^2 = \frac{\sum_{j=1}^p (n_j -1) s_j^2}{\sum_{j=1}^p (n_j - 1)} = \frac{(n_1-1) s_1^2 + (n_2 -1) s_2^2}{n_1 + n_2 -2}
			\end{gather*}
		\end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}{If $H_0$ were true}
	\begin{itemize}
		\item $s_B^2$: Between-group variance 
		\begin{itemize}
			\item If $H_0$ were true, than the sample mean  $\bar{y}_j$ of each of the independent samples from the $p$ groups would be an estimate of the same population mean because $\mu_1 = \mu_2 = \hdots = \mu_p$
			\item[]
			\item We could then compute the sample variance of this set of $p$ sample means: $\{\bar{y}_1, \bar{y}_2, \hdots \bar{y}_p\}$:
			\begin{gather*}
				\frac{\sum_{j=1}^p (\bar{y}_j - \bar{y}.)^2}{p-1}
			\end{gather*}
			where $\bar{y}. = \frac{\sum_{j=1}^p \bar{y}_j}{p}$
			\item[]
			\item If $n_j$ is equal for each group, then the above quantity is an estimate of $\sigma^2 / n$. So $n_j$ times that quantity estimates $\sigma^2$
		\end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}{If $H_0$ were true}
	\begin{itemize}
		\item Now a test statistic!
		\item[]
		\item If the null hypothesis is true, then $s_B^2$ and $s_W^2$ both estimate $\sigma^2$ and should both be close to eachother, so:
		\begin{gather*}
		F^* = s_B^2 / s_W^2
		\end{gather*}
		Should be close to 1 and has an $F$ distribution
	\end{itemize}
\end{frame}

\begin{frame}{If $H_0$ was not true}
	\begin{itemize}
		\item If $H_0$ were not true then we would expect that $s^2_B$ will be greater than $s_W^2$. That is, the between group variability will be greater than the within group variability 
		\item[]
		\item So this would put our test statistic $F^*$ in the right tail of the $F$ distribution under the null
		\item[]
		\item We will call this testing Analysis of Variance (or ANOVA or AOV)
	\end{itemize}
\end{frame}

\begin{frame}{Break for data}
	\begin{center}
		\includegraphics[width=.9\linewidth]{Trees1}
	\end{center}
\end{frame}

\begin{frame}{Break for data}
	\begin{itemize}
		\item Before we introduce some notation and the process for doing an ANOVA $F$-test let's introduce some data
	\end{itemize}
\begin{center}
	\includegraphics{treeData2}
\end{center}
\end{frame}

\begin{frame}{Break for data}
	\begin{center}
		\includegraphics[width=.45 \linewidth]{treeData}
	\end{center}
\end{frame}

\begin{frame}{Completely randomized designs}
	\begin{itemize}
		\item We will now talk about completely randomized designs. In a completely randomized design we have multiple populations whose means we want to compare, and we have a random sample of observations from each
	\end{itemize}
\end{frame}

\begin{frame}{Notation}
	\begin{itemize}
		\item $k$: The number of groups (populations)
		\item[]
		\item $y_{j,i}:$ The $i$th sample observation from the group (population) $j$. For example, $y_{3,2}$ in our data would be the dbh of Shortleaf Pine \#2
		\item[]
		\item $n_j$: The number of observations from group (population) $j$. For our data $n_j=10$ for all groups $j$
		\item[]
		\item $N$: The total sample size. That is $N = \sum_{j=1}^{k} n_j$. For our data $N = 30$
		\item[]
		\item $\bar{y}_{j.}$: The mean of the $n_j$ observations from group (population) $j$. That is $\bar{y}_{j.} = \frac{\sum_{i = 1}^{n_j} y_{j,i}}  {n_j}$
		\item[]
		\item $\bar{y}_{..}$: The mean of all of the observations $\sum_{j=1}^k\sum_{i = 1}^{n_j} y_{j,i} / n_j$
	\end{itemize}
\end{frame}

\begin{frame}{Sum of squares}{Total sum of squares (TSS)}
	\begin{itemize}
		\item We are now going to introduce how $s^2_B$, $s^2_W$, and the ANOVA $F$ test statistic are computed in practice 
		\item[]
		\item \textbf{Total sum of squares (TSS).} The total sum of squares is the variability of the sample measurements about the overall mean (of all measurements):
		\begin{gather*}
			\text{TSS} = \sum_{j=1}^{k} \sum_{i = 1}^{n_j}\left(y_{j,i} - \bar{y}_{..}\right)
		\end{gather*}
	\end{itemize}
\end{frame}

\begin{frame}{Sum of squares}{Partitioning TSS}
	\begin{itemize}
		\item We can partition the TSS as follows:
		\begin{gather*}
			\sum_{j=1}^{k} \sum_{i = 1}^{n_j}\left(y_{j,i} - \bar{y}_{..}\right) = \sum_{j=1}^{k}\sum_{i = 1}^{n_j} (y_{j,i}-\bar{y}_{j.})^2 + \sum_{j=1}^k n_j (\bar{y}_{j.} - \bar{y}_{..})^2
		\end{gather*}
		\item[]
		\item This is:
		\begin{gather*}
			\text{TSS} =\sum_{j=1}^{k} \sum_{i = 1}^{n_j}\left(y_{j,i} - \bar{y}_{..}\right) = \text{SSW} + \text{SSB}
		\end{gather*}
		if $\text{SSW} = \sum_{j=1}^{k}\sum_{i = 1}^{n_j} (y_{j,i}-\bar{y}_{j.})^2$ and $\text{SSB}= \sum_{j=1}^k n_j (\bar{y}_{j.} - \bar{y}_{..})^2$
	\end{itemize}
\end{frame}

\begin{frame}{Sum of Squares}{Within-sample sum of squares}
	\begin{itemize}
		\item \textbf{Within-sample sum of squares (SSW).} A measure of within-sample variability:
		\begin{gather*}
			\text{SSW} = \sum_{j=1}^{k}\sum_{i = 1}^{n_j} (y_{j,i}-\bar{y}_{j.})^2 = (n_1-1)s_1^2 + (n_2-1)s_2^2 + \hdots + (n_k-1)s_k^2
		\end{gather*}
		\item \textbf{Sum of squares between samples (SSB).} A measure of variability between (or among) the sample means:
		\begin{align*}
			\text{SSB} =\sum_{j=1}^k n_j (\bar{y}_{j.} - \bar{y}_{..})^2 = n_1 (\bar{y}_{1.}-\bar{y}_{..})^2 + &n_2 (\bar{y}_{2.}-\bar{y}_{..})^2 + \hdots + \\ &n_k (\bar{y}_{k.}-\bar{y}_{..})^2
		\end{align*}
	\end{itemize}
\end{frame}

\begin{frame}{Mean squares}
	\begin{itemize}
		\item The quantities $s^2_B$ and $s^2_W$ are (historically) referred to as mean squares (as in mean square between samples and mean square within samples, respectively):
		\begin{gather*}
			s^2_B = \frac{\text{SSB}}{k-1} \quad \quad s^2_W = \frac{\text{SSW}}{N-k}
		\end{gather*}
		where $k-1$ is the degrees of freedom of $s^2_B$ and $N-k$ is the degrees of freedom of $s^2_W$
	\end{itemize}
\end{frame}

\end{document}