\documentclass[xcolor=dvipsnames]{beamer}
\usetheme{AnnArbor}
\usecolortheme{beaver}

\usepackage{amsmath,graphicx,booktabs,tikz,subfig,color,lmodern}
\definecolor{mycol}{rgb}{.4,.85,1}
\setbeamercolor{title}{bg=mycol,fg=black} 
\setbeamercolor{palette primary}{use=structure,fg=white,bg=red}
\setbeamercolor{block title}{fg=white,bg=red!50!black}
% \setbeamercolor{block title}{fg=white,bg=blue!75!black}

\newcommand\myeq{\mathrel{\overset{\makebox[0pt]{\mbox{\normalfont\tiny\sffamily D}}}{=}}}

\title[Lecture 19]{Lecture 19: Linear Regression: Prediction and Diagnostics}
\author[Patrick Trainor]{Patrick Trainor, PhD, MS, MA}
\institute[NMSU]{New Mexico State University}
\date{November 18, 2019}

\begin{document}
\begin{frame}
\maketitle
\end{frame}

\begin{frame}{Outline}
\tableofcontents[hideallsubsections]
\end{frame}

\section{Model diagnostics (Part 1)}
\begin{frame}{Outline}
\tableofcontents[currentsection,subsectionstyle=show/shaded/hide]
\end{frame}

\begin{frame}
	\begin{itemize}
		\item When we estimate a linear regression model, we want to know how good the model fits the data
		\item[]
	\end{itemize}
\end{frame}

\begin{frame}{Leverage and Influence}
	\begin{itemize}
		\item \textbf{High leverage point:} A point in the dataset that is an outlier in $x$, has greater weight in the estimation of the slope \pause
		\item[]
		\item \textbf{High influence point:} A point that is a high leverage point and also an outlier in $y$. The great weight it has in the estimation of the slope will dramatically alter the estimate of the slope (inappropriately)
	\end{itemize}
\end{frame}

\begin{frame}{Leverage and Influence}
A high leverage point: $\hat{\beta}_1$ changes from 2.0875 to 2.1378
	\begin{center}
		\includegraphics[width=.7\linewidth]{../lecture18_LinearRegression/p0_10}
	\end{center}
\end{frame}

\begin{frame}{Leverage and Influence}
A high influence point: $\hat{\beta}_1$ changes from 2.0875 to 1.0954
\begin{center}
	\includegraphics[width=.7\linewidth]{../lecture18_LinearRegression/p0_11}
\end{center}
\end{frame}

\begin{frame}{Leverage and Influence}
A high influence point: $\hat{\beta}_1$ changes from 2.0875 to -0.3938
\begin{center}
	\includegraphics[width=.7\linewidth]{../lecture18_LinearRegression/p0_12}
\end{center}
\end{frame}

\begin{frame}{The accuracy of $\hat{\beta}_1$}
	\begin{itemize}
		\item The standard error of $\hat{\beta}_1$ informs us how accurately we can estimate the population value $\beta_1$ \pause
		\item[]
		\item From $\sigma_{\hat{\beta}_1} = \frac{\sigma_{\varepsilon}}{\sqrt{S_{xx}}}$ we can see that:
		\begin{itemize}
			\item The less variability there is in $y$ for a given value of $x$, the more accurately we can estimate $\beta_1$ \pause
			\item[]
			\item The greater $S_{xx}$, the more accurately we can estimate $\beta_1$ \pause
			\item[]
			\item ``The slope is the predicted change in y per unit change in $x$; if $x$ changes very little in the data, so that $S_{xx}$ is small, it is difficult to estimate the rate of change in $y$ accurately''
		\end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}{The accuracy of $\hat{\beta}_0$}
	\begin{itemize}
		\item The standard error of $\hat{\beta}_0$ informs us how accurately we can estimate the population value $\beta_0$ \pause
		\item[]
		\item From $\sigma_{\hat{\beta}_0} = \sigma_{\varepsilon} \sqrt{\frac{1}{n}+\frac{\bar{x}^2}{S_{xx}}}$ we can see that: \pause
		\begin{itemize}
			\item The greater $\bar{x}^2$ is, the less accuracy we have in estimating $\beta_0$ \pause
			\item[]
			\item $\beta_0$ is the $y$ value for $x = 0$. Consequently if most of the $x$ values are far away from zero, it is hard to estimate $y$ at $x = 0$
		\end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}{The accuracy of $\hat{\beta}_0$}
\begin{center}
	\includegraphics[width=.75\linewidth]{../lecture18_LinearRegression/p9Beta0}
\end{center}
\end{frame}

\begin{frame}{Sum of Squares}
	\begin{itemize}
		\item Recall that: 
		\begin{gather*}
		s^2_{\varepsilon} = \frac{\sum_{i=1}^{n}\left(y_i - \hat{y}_i\right)^2}{n-2} = \frac{\text{SS}(\text{Error})}{n-2}
		\end{gather*} 
		is an estimate of $\sigma_{\varepsilon}^2$ \pause
		\item[]
		\item We have the following sum of squares: \pause
		\begin{itemize}
			\item Error or residual sum of squares: $\sum_{i=1}^n (y_i - \hat{y}_i)^2$ \pause
			\item[]
			\item Model or regression sum of squares: $\sum_{i=1}^n (\hat{y}_i-\bar{y})^2$ \pause
			\item[]
			\item Total sum of squares: $\sum_{i=1}^n (y_i -\bar{y})^2 = \text{SS}(\text{Regression})+\text{SS}(\text{Error})$
		\end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}{A Sum of Squares $F$-test}
	\begin{itemize}
		\item If we determine the ratio of the model/regression mean square to the error or residual mean square it tells us the ratio of the variability that is accounted for by the model, versus that which is not \pause
		\item[]
		\item Under the null hypothesis that the model has no predictive value, this ratio will have an $F$-distribution \pause
		\item[]
		\item For our simple linear model, ``no predictive value'' corresponds to $\beta_1 = 0$
	\end{itemize}
\end{frame}

\begin{frame}{Regression model $F$-test}
	\begin{itemize}
		\item Hypotheses. $H_0: \beta_1 = 0$ versus $H_a: \beta_1 \neq 0$ \pause
		\item[]
		\item Test statistic: \pause
		\begin{gather*}
		F^*= \frac{\text{MS}(\text{Regression})}{\text{MS}(\text{Error})}
		\end{gather*} \pause
		\item Rejection rule: Reject $H_0$ if $F^*> F_{\alpha}$ given $\text{df}_1=1$ and $\text{df}_2=n-2$ \pause
		\item[]
		\item p-value: $P(F > F^*)$ given $\text{df}_1=1$ and $\text{df}_2=n-2$ \pause
		\item[]
		\item This test will be equivalent to the $t$-test regarding $\beta_1$ for simple linear regression
	\end{itemize}
\end{frame}

\begin{frame}{$F$-test example}
	\begin{itemize}
		\item Back to the example of predicting SBP from cortisol level: \pause
		\begin{itemize}
			\item Regression sum of squares: 157.347 \pause
			\item $\text{MS}(\text{Regerssion}) = 157.347 / 1 = 157.347$ \pause
			\item Error sum of squares: 76.034 \pause
			\item $\text{MS}(\text{Error}) = 76.034 / 6 = 12.672$ \pause
			\item[]
		\end{itemize}
		\item So for testing whether the model has predictive value ($H_a$), versus it does not, $H_0$: \pause
		\begin{gather*}
			F^*= \frac{\text{MS}(\text{Regression})}{\text{MS}(\text{Error})} = \frac{157.347}{12.672} = 12.417
		\end{gather*} \pause
		\item The p-value for this test is $P(F \geq F^*) = P(F \geq 12.417) = 0.01246$,  given $\text{df}_1=1$ and $\text{df}_2=n-2 = 6$
	\end{itemize}
\end{frame}

\begin{frame}{$F$-test example}{Using R}
	\begin{center}
		\includegraphics[width=.8\linewidth]{../lecture18_LinearRegression/Rlm4}
	\end{center}
\end{frame}

\begin{frame}{$F$-test example}{Using R}
	\begin{center}
		\includegraphics[width=.8\linewidth]{../lecture18_LinearRegression/Rlm5}
	\end{center}
\end{frame}

\begin{frame}{$F$-test example}{Using SAS}
	\begin{center}
		\includegraphics[width=.6\linewidth]{../lecture18_LinearRegression/SAS1_6}
	\end{center}
\end{frame}

\section{Prediction}

\begin{frame}{Outline}
\tableofcontents[currentsection,subsectionstyle=show/shaded/hide]
\end{frame}

\begin{frame}{Prediction}
	\begin{itemize}
		\item Prediction: \pause
		\begin{itemize}
			\item Thus far we have considered modeling the relationship between $x$ and $y$, given a sample from a population \pause
			\item[]
			\item Often our goal is to determine what the values of $y$ are likely to be for $x$ values that we haven't yet observed \pause
			\item[]
			\item Example: From our example of using cortisol to predict SBP values, we used a sample to determine a model. Now we may want to use that model to predict the SBP value from cortisol for a new person \pause
			\item[]
		\end{itemize}
		\item Other examples: \pause
	\begin{itemize}
		\item A bank wants to determine who is likely to default on a loan based on the characteristics of customers who have defaulted in the past \pause
		\item[]
		\item A hydrologist might want to predict the amount of water held in a reservoir in summer from measurements of winter snowpack (using data from the past to estimate the relationship)
	\end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}{Prediction}
	\begin{itemize}
		\item Two types of prediction tasks: \pause
		\begin{enumerate}
			\item Predicting the mean of $y$ given $x$, that is $E(y)$ given $x$ \pause
			\item Predicting individual $y$ values given $x$ \pause
			\item[]
		\end{enumerate}
		\item Example. From before we had that $\hat{\text{SBP}} = 92.3177 + 2.0875 \times \text{Cortisol}$ \pause
		\begin{enumerate}
			\item Predicting the average of $SBP$ values for the population with $\text{Cortisol} = 15$: \pause
			\begin{gather*}
				E(\text{SBP}) = 92.3177 + 2.0875(15)
			\end{gather*} \pause
			\item Predicting the specific $SBP$ value for an individual with $\text{Cortisol} = 15$: \pause
			\begin{gather*}
				\hat{\text{SBP}} = 92.3177 + 2.0875(15)
			\end{gather*}
		\end{enumerate}
	\end{itemize}
\end{frame}

\begin{frame}{Prediction}
	\begin{center}
		\includegraphics[width=.7\linewidth]{../lecture18_LinearRegression/IMG_20191104_224711}
	\end{center}
\end{frame}

\begin{frame}{Point estimate and CI for $E(y_{\text{new}})$}
	\begin{itemize}
		\item For estimating the mean (or expected value) of $y$ given a new $x$, we have: $E(y_{\text{new}}) = \hat{\beta}_0 + \hat{\beta}_1 x_{\text{new}}$ \pause
		\item[]
		\item The standard error of $E(y_{\text{new}})$ is: \pause
		\begin{gather*}
			\sigma_{\varepsilon} \sqrt{\frac{1}{n} + \frac{(x_{\text{new}}-\bar{x})^2}{S_{xx}}}
		\end{gather*} \pause
		\item This gives us a $100(1-\alpha)\%$ confidence interval for $E(y_{\text{new}})$: \pause
		\begin{gather*}
		\hat{y}_{\text{new}} \pm t_{\alpha / 2, \text{df} = n-2}s_{\varepsilon}\sqrt{\frac{1}{n} + \frac{(x_{\text{new}}-\bar{x})^2}{S_{xx}}}
		\end{gather*}
	\end{itemize}
\end{frame}

\begin{frame}{Point estimate and CI for $E(y_{\text{new}})$}
	\begin{itemize}
		\item In our model for predicting SBP from cortisol we had that: $s_{\varepsilon} = 3.560$, $S_{xx} = 36.1084$ and $\bar{x}=13.17$ \pause
		\item[]
		\item A point estimate for the average value of $y$ (SBP), given $x=15$ ($\text{Cortisol}=15$) is: \pause
		\begin{gather*}
			92.3177 + 2.0875(15) = 123.6302
		\end{gather*}
		\item A 95\% confidence interval for the average value of $y$ (SBP) given $x=15$:  ($\text{Cortisol}=15$) is: \pause
		\begin{gather*}
			\hat{y}_{\text{new}} \pm t_{\alpha / 2, \text{df} = n-2}s_{\varepsilon}\sqrt{\frac{1}{n} + \frac{(x_{\text{new}}-\bar{x})^2}{S_{xx}}} \\
			123.6302 \pm 2.447 (3.560)\sqrt{\frac{1}{8} + \frac{(15-13.17)^2}{36.1084}} = (119.5652, 127.6952)
		\end{gather*}
	\end{itemize}
\end{frame}

\begin{frame}{Point estimate and CI for $E(y_{\text{new}})$}
\begin{center}
	\includegraphics[width=.7\linewidth]{../lecture18_LinearRegression/p9Conf}
\end{center}
\end{frame}

\begin{frame}{Point estimate and CI for $E(y_{\text{new}})$}{Extrapolation Penalty}
	\begin{center}
		\includegraphics[width=.75\linewidth]{../lecture18_LinearRegression/p9Confb}
	\end{center}
\end{frame}

\begin{frame}{Point estimate and CI for $E(y_{\text{new}})$}{Extrapolation Penalty}
\begin{center}
	\includegraphics[width=.75\linewidth]{../lecture18_LinearRegression/p9Confc}
\end{center}
\end{frame}

\begin{frame}{Point estimate and CI for $E(y_{\text{new}})$}{Extrapolation Penalty}
\begin{itemize}
	\item \textbf{Extrapolation penalty:} The term $\frac{(x_{\text{new}}-\bar{x})^2}{S_{xx}}$ in $s_{\varepsilon}\sqrt{\frac{1}{n} + \frac{(x_{\text{new}}-\bar{x})^2}{S_{xx}}}$ increases the width of the confidence interval \pause
	\item[]
	\item So the further away $x_{\text{new}}$ is from $\bar{x}$, the greater the width of the confidence interval of $E(y_{\text{new}})$
\end{itemize}
\end{frame}

\begin{frame}{Point estimate and CI for $E(y_{\text{new}})$}{R example}
	\begin{center}
		\includegraphics[width=.7\linewidth]{../lecture18_LinearRegression/Rlm6}
	\end{center}
\end{frame}

\begin{frame}{Point estimate and CI for $E(y_{\text{new}})$}{R example}
	\begin{center}
		\includegraphics[width=1\linewidth]{../lecture18_LinearRegression/Rlm7}
	\end{center}
\end{frame}

\begin{frame}{Point estimate and CI for $E(y_{\text{new}})$}{SAS example}
	\begin{center}
		\includegraphics[width=.7\linewidth]{../lecture18_LinearRegression/sas1_7}
	\end{center}
\end{frame}

\begin{frame}{Point estimate and CI for $E(y_{\text{new}})$}{SAS example}
	\begin{center}
		\includegraphics[width=.55\linewidth]{../lecture18_LinearRegression/sas1_8}
	\end{center}
\end{frame}

\begin{frame}{Point estimate and CI for $y_{\text{new}}$}
	\begin{itemize}
		\item The point estimate for $y_{\text{new}}$ given $x_{\text{new}}$ is $\hat{y}=\hat{\beta}_0 + \hat{\beta}_1 x_{\text{new}}$ \pause
		\item[]
		\item The formula for a $100(1-\alpha)\%$: \pause
		\begin{gather*}
			\hat{y}_{\text{new}} \pm t_{\alpha / 2, \text{df} = n-2}s_{\varepsilon}\sqrt{1+\frac{1}{n} + \frac{(x_{\text{new}}-\bar{x})^2}{S_{xx}}}
		\end{gather*}
	\end{itemize}
\end{frame}

\begin{frame}{Point estimate and CI for $y_{\text{new}}$}{Example}
\begin{itemize}
\item In our model for predicting SBP from cortisol we had that: $s_{\varepsilon} = 3.560$, $S_{xx} = 36.1084$ and $\bar{x}=13.17$ \pause
\item[]
\item A point estimate for $y$ (SBP), given $x=15$ ($\text{Cortisol}=15$) is: \pause
\begin{gather*}
92.3177 + 2.0875(15) = 123.6302
\end{gather*} \pause
\item A 95\% confidence interval for the value of $y$ (SBP) given $x=15$ ($\text{Cortisol}=15$) is: \pause
	\begin{gather*}
	\hat{y}_{\text{new}} \pm t_{\alpha / 2, \text{df} = n-2}s_{\varepsilon}\sqrt{1+\frac{1}{n} + \frac{(x_{\text{new}}-\bar{x})^2}{S_{xx}}} \\
	123.6302 \pm 2.447 (3.560)\sqrt{1+ \frac{1}{8} + \frac{(15-13.17)^2}{36.1084}} = \\
	(114.0171, 133.2433)
	\end{gather*}
\end{itemize}
\end{frame}

\begin{frame}{Point estimate and CI for $y_{\text{new}}$}{R example}
	\begin{center}
		\includegraphics[width=1\linewidth]{../lecture18_LinearRegression/Rlm8}
	\end{center}
\end{frame}

\begin{frame}{Point estimate and CI for $y_{\text{new}}$}{SAS example}
	\begin{center}
		\includegraphics[width=.7\linewidth]{../lecture18_LinearRegression/SAS1_9}
	\end{center}
\end{frame}

\begin{frame}{Point estimate and CI for $y_{\text{new}}$}{SAS example}
	\begin{center}
		\includegraphics[width=.6\linewidth]{../lecture18_LinearRegression/SAS1_10}
	\end{center}
\end{frame}


\section{Model diagnostics (Part 2)}
\begin{frame}{Outline}
\tableofcontents[currentsection,subsectionstyle=show/shaded/hide]
\end{frame}

\begin{frame}{Lack of fit in linear regression}{Plot of $y_i - \hat{y}_i$ versus $\hat{y}_i$}
	\begin{itemize}
		\item We started by assuming a particular form of model relating $x$ and $y$, that is: $y = \beta_0 + \beta_1 x + \varepsilon$ \pause
		\item[]
		\item But how do we know if that form is ``good'' or not? \pause
		\item[]
		\item A good starting point is a scatterplot of the residuals versus predicted values, that is $y_i - \hat{y}_i$ versus $\hat{y}_i$ 
	\end{itemize}
\end{frame}

\begin{frame}{Formal assumptions of simple linear regression}
\begin{enumerate}
	\item The relationship between $x$ and $y$ is linear
	\item[]
	\item $E(\varepsilon_i) = 0$ for all $i$
	\item[]
	\item $Var(\varepsilon_i) = \sigma^2$ for all $i$
	\item[]
	\item The errors are independent of each other
	\item[]
	\item The errors are all normally distributed. $\varepsilon_i$ is normally distributed for all $i$
\end{enumerate}
\end{frame}

\begin{frame}{An appropriate model}{Regression line}
\begin{itemize}
	\item Relationship between $x$ and $y$ is linear; homogeneous variance 
\end{itemize}
	\begin{center}
		\includegraphics[width=.7\linewidth]{../lecture18_LinearRegression/p10}
	\end{center}
\end{frame}

\begin{frame}{An appropriate model}{Plot of $y_i - \hat{y}_i$ versus $\hat{y}_i$}
\begin{itemize}
	\item Relationship between $x$ and $y$ is linear; homogeneous variance
\end{itemize}
\begin{center}
	\includegraphics[width=.7\linewidth]{../lecture18_LinearRegression/p11}
\end{center}
\end{frame}

\begin{frame}{Not an appropriate model}{Regression line}
\begin{itemize}
	\item Relationship between $x$ and $y$ is not linear; homogeneous variance
\end{itemize}
\begin{center}
	\includegraphics[width=.7\linewidth]{../lecture18_LinearRegression/p10a}
\end{center}
\end{frame}

\begin{frame}{Not an appropriate model}{Plot of $y_i - \hat{y}_i$ versus $\hat{y}_i$}
\begin{itemize}
\item Relationship between $x$ and $y$ is not linear; homogeneous variance
\end{itemize}
\begin{center}
\includegraphics[width=.7\linewidth]{../lecture18_LinearRegression/p11a}
\end{center}
\end{frame}

\begin{frame}{Not an appropriate model}{Regression line}
\begin{itemize}
	\item Relationship between $x$ and $y$ is linear; non-homogeneous variance
\end{itemize}
\begin{center}
	\includegraphics[width=.7\linewidth]{../lecture18_LinearRegression/p10b}
\end{center}
\end{frame}

\begin{frame}{Not an appropriate model}{Plot of $y_i - \hat{y}_i$ versus $\hat{y}_i$}
\begin{itemize}
\item Relationship between $x$ and $y$ is linear; non-homogeneous variance
\end{itemize}
\begin{center}
\includegraphics[width=.7\linewidth]{../lecture18_LinearRegression/p11b}
\end{center}
\end{frame}

\begin{frame}{Example}
\begin{itemize}
	\item A real estate analytics company wants to determine the relationship between the size of single family homes and the sales price
\end{itemize}
	\begin{center}
	\includegraphics[width = .6\linewidth]{../lecture18_LinearRegression/p12}
\end{center}
\end{frame}

\begin{frame}{Example}
	\begin{itemize}
		\item A real estate analytics company wants to determine the relationship between the size of single family homes and the sales price
	\end{itemize}
	\begin{center}
		\includegraphics[width = .6\linewidth]{../lecture18_LinearRegression/p13}
	\end{center}
\end{frame}

\begin{frame}{Example}
	\begin{center}
		\includegraphics[width = .95\linewidth]{../lecture18_LinearRegression/lm3}
	\end{center}
\end{frame}

\begin{frame}{Example}
	\begin{itemize}
		\item A real estate analytics company wants to determine the relationship between the size of single family homes and the sales price
	\end{itemize}
	\begin{center}
		\includegraphics[width = .6\linewidth]{../lecture18_LinearRegression/p14}
	\end{center}
\end{frame}

\section{Variable transformations}
\begin{frame}{Outline}
	\tableofcontents[currentsection,subsectionstyle=show/shaded/hide]
\end{frame}

\begin{frame}{Transformation of $y$}
	\begin{itemize}
		\item Sometimes when the assumption $Var(\varepsilon_i)$ for all $i$ is not met there may be an appropriate transformation of $y$, yielding $y_{trans}$ such that this assumption is met \pause
		\item[]
		\item Possible transformations: \pause
		\begin{center}
			\begin{tabular}{p{6cm}p{4cm}}
				\textbf{Relationship between $\mu$ and $\sigma^2$} & $y_{trans}$ \\ \hline
				$\sigma^2 = k\mu $ & $y_{trans} = \sqrt{y}$ \\
				$\sigma^2 = k\mu^2 $ & $y_{trans} = \log(y)$ \\ \hline
			\end{tabular}
		\end{center}
	\end{itemize}
\end{frame}

\begin{frame}{Transformation of $y$}{Example}
	\begin{itemize}
		\item In predicting price from size (Square feet), we could try a log-transform, that is $y_{trans} = \log(y)$: \pause
		\begin{center}
			\includegraphics[width=.6\linewidth]{../lecture18_LinearRegression/p12Trans}
		\end{center}
	\end{itemize}
\end{frame}

\begin{frame}{Transformation of $y$}{Example}
	\begin{itemize}
		\item In predicting price from size (Square feet), we could try a log-transform, that is $y_{trans} = \log(y)$:
		\begin{center}
			\includegraphics[width=.6\linewidth]{../lecture18_LinearRegression/p14Trans}
		\end{center}
	\end{itemize}
\end{frame}

\begin{frame}{Transformation of $y$}{Example}
	\begin{center}
		\includegraphics[width = .9\linewidth]{../lecture18_LinearRegression/lm3Trans}
	\end{center}
\end{frame}

\begin{frame}{Transformation of $y$}
	\begin{itemize}
		\item In the predicting price from home size example, the log transformation \emph{appeared} (although we don't formally learn the test for this) to improve the variance assumptions \pause
		\item[]
		\item The transformation also improved the model fit (check the model $F$-statistic) \pause
		\item[]
		\item Model interpretation is more difficult: \pause
		\begin{itemize}
			\item Model without transformation: $\hat{\text{Price}} = 40843.07 + 126.93(\text{Square Feet})$ \pause
			\item Model with transformation: $\log(\hat{\text{Price}}) = 11.70867 + 0.0003631(\text{Square Feet})$
		\end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}{Transformation of $y$}
	\begin{itemize}
		\item Another way to interpret results with a transformation: \pause
		\begin{gather*}
			\log(\hat{y}) = \hat{\beta}_0 + \hat{\beta}_1x
		\end{gather*}
		implies that: \pause
		\begin{align*}
			\hat{y} &= e^{\hat{\beta}_0 + \hat{\beta}_1x } = e^{\hat{\beta}_0} (e^{\hat{\beta}_1})^x
		\end{align*}
		So: \pause
		\begin{align*}
			\frac{\hat{y}_{x+1}}{\hat{y}_{x}}&=\frac{e^{\hat{\beta}_0} (e^{\hat{\beta}_1})^{(x+1)}}{e^{\hat{\beta}_0} (e^{\hat{\beta}_1})^x} = e^{\hat{\beta}_1}
		\end{align*} \pause
		\item So every time $x$ goes up by 1 unit, $y$ goes up by a \emph{multiplicative} factor of $(e^{\hat{\beta}_1})$
	\end{itemize}
\end{frame}

\begin{frame}{Transformation of $y$}{Example}
	\begin{itemize}
		\item Every time $x$ goes up by 1 unit, $y$ goes up by a \emph{multiplicative} factor of $(e^{\hat{\beta}_1})$
		\item[]
		\item Our model predicting price from size: $\log(\hat{\text{Price}})=11.70867 + 0.0003631(\text{Square Feet})$ \pause
		\begin{itemize}
			\item If $\text{Square Feet} = 1120$, then $\text{Price}=182653.5$ \pause
			\item[]
			\item If $\text{Square Feet} = 1121$, then $\text{Price}=182719.8$ \pause
			\item[]
			\item And: \pause
			\begin{gather*}
				\frac{182653.5}{182719.8}=1.000363 = e^{0.00036310} = e^{\hat{\beta}_1}
			\end{gather*}
		\end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}{Transformation of $x$}
	\begin{itemize}
		\item Occasionally, transforming $x$ may help ensure that the assumptions of simple linear regression are met
		\item[]
		\item Some common transformations for $x$ are: $x^2$, $\log(x)$, and $\sqrt{x}$
	\end{itemize}
\end{frame}

\begin{frame}{Transformation of $x$}{Example}
	\begin{itemize}
		\item \textbf{Example.} A company in Elkhart, Indiana produces recreational vehicles. Their factory is designed to produce a specific number of units each month. The company wants to determine if producing more or less than this optimal number of units impacts the profit margin on each unit. They observe the data on the following slide
	\end{itemize}
\end{frame}

\begin{frame}{Transformation of $x$}{Example}
	\begin{center}
		\includegraphics[width=1\linewidth]{../lecture18_LinearRegression/df7}
	\end{center}
\end{frame}

\begin{frame}{Transformation of $x$}{Example}
	\begin{center}
		\includegraphics[width = .7\linewidth]{../lecture18_LinearRegression/nonLinear1}
	\end{center}
\end{frame}

\begin{frame}{Transformation of $x$}{Example}
	\begin{center}
		\includegraphics[width=1\linewidth]{../lecture18_LinearRegression/df7b}
	\end{center}
\end{frame}

\begin{frame}{Transformation of $x$}{Example}
	\begin{center}
		\includegraphics[width=1\linewidth]{../lecture18_LinearRegression/df7c}
	\end{center}
\end{frame}

\begin{frame}{Transformation of $x$}{Example}
	\begin{center}
		\includegraphics[width = .7\linewidth]{../lecture18_LinearRegression/nonLinear2}
	\end{center}
\end{frame}

\begin{frame}{The end of Lecture \#19}
	\begin{center}
		\includegraphics[width = .75\linewidth]{33v2}
	\end{center}
\end{frame}

\end{document}