\documentclass[xcolor=dvipsnames]{beamer} 
\usetheme{AnnArbor}
\usecolortheme{beaver}

\usepackage{amsmath,graphicx,booktabs,tikz,subfig,color,lmodern}
\definecolor{mycol}{rgb}{.4,.85,1}
\setbeamercolor{title}{bg=mycol,fg=black} 
\setbeamercolor{palette primary}{use=structure,fg=white,bg=red}
\setbeamercolor{block title}{fg=white,bg=red!50!black}
% \setbeamercolor{block title}{fg=white,bg=blue!75!black}

\title[Lecture 8]{Lecture 8: Inferences about the mean}
\author[Patrick Trainor]{Patrick Trainor, PhD, MS, MA}
\institute[NMSU]{New Mexico State University}
\date{}

\begin{document}

\begin{frame}
	\maketitle
\end{frame}

\begin{frame}{Outline}
	\tableofcontents[hideallsubsections]
\end{frame}

\section{Introduction}
\begin{frame}{Outline}
	\tableofcontents[currentsection,subsectionstyle=show/shaded/hide]
\end{frame}

\begin{frame}{Introduction}
	\begin{columns}
		\begin{column}{.4 \textwidth}
			\begin{center}
				\includegraphics[width=1.2 \linewidth]{./../lecture1_Introduction/Sampling.png}
				\end{center}
		\end{column}
		\begin{column}{.5 \textwidth}
				\begin{itemize}
				\item All the way back in Lecture \#1 we discussed how we use samples to make inferences about larger populations \pause
				\item[]
				\item In general, the two types of inference we want to make are: \pause
				\begin{itemize}
					\item an estimate of the value of a population parameter \pause
					\item[]
					\item a test of a hypothesis about the value of a parameter(s)
				\end{itemize}
			\end{itemize}
		\end{column}
	\end{columns}
\end{frame}

\begin{frame}{Introduction}{Inference examples}
	\begin{columns}
		\begin{column}{.5 \textwidth}
			Population parameter estimations:
			\begin{itemize}
				\item What is the prevalence of Type 2 Diabetes in the United States (or in New Mexico, or in Las Cruces)? \pause
				\item[]
				\item What percent of middle school-aged youth are bullied online? \pause
				\item[]
				\item What percentage of \emph{Ixodes scapularis} (deer ticks) carry the bacterium \emph{Borrelia burgdorferi} in Kentucky? \pause
			\end{itemize}
		\end{column}
	
		\begin{column}{.5 \textwidth}
			Test of hypotheses about the value of parameters:
			\begin{itemize}
				\item Is the prevalence of Type 2 Diabetes in the United States greater than 50 million people? \pause 
				\item[]
				\item Are more than 10\% of middle school-aged youth bullied online? \pause
				\item[]
				\item Does treatment of clothing with permethrin reduce incidence of lyme disease? 
			\end{itemize}
		\end{column}
	\end{columns}
\end{frame}

\section{Estimation of Normal population mean}
\begin{frame}{Outline}
	\tableofcontents[currentsection,subsectionstyle=show/shaded/hide]
\end{frame}

\begin{frame}{The sample mean}
	\begin{itemize}
		\item There is an entire field of mathematical statistics devoted to determining the best estimator for different types of population parameters \pause
		\item[]
		\item For Normal distributions, the sample mean is the ``best'' estimate (or estimator) of the population mean \pause
		\item[]
		\item That the sample mean is a good guess of the value of the population mean is intuitive, but an important question remains...\emph{How confident should we be in the reliability of the sample mean as an estimate of the population mean?}
	\end{itemize}
\end{frame}

\begin{frame}{The sample mean}
	\begin{itemize}
		\item Let's play a guessing game!
		\item[] 
		\item Sally says that the average high temperature in Louisville, Kentucky in September is between 84 and 84.5. Paul says that the average is between 0 and 150 degrees
	\end{itemize}
\begin{center}
	\includegraphics[width=.7\linewidth]{Louisville}
\end{center}
\end{frame}

\begin{frame}{The sample mean}
	\begin{itemize}
		\item Sally says that the average high temperature in Louisville, Kentucky in September is between 84 and 84.5. Paul says that the average is between 0 and 150 degrees
		\begin{itemize}
					\item Both are providing interval estimates for the population mean
			\item These are needed because the probability of perfectly estimating a population parameter is usually zero
		\end{itemize}
	\end{itemize}
	\begin{center}
		\includegraphics[width=.7\linewidth]{Louisville}
	\end{center}
\end{frame}

\begin{frame}{The sample mean}
	\begin{itemize}
		\item Sally says that the average high temperature in Louisville, Kentucky in September is between 84 and 84.5. Paul says that the average is between 0 and 150 degrees.
		\item[]
		\item Who is more ``right'' if the true (population) value is 82 degrees?
	\end{itemize}
	\begin{center}
		\includegraphics[width=.7\linewidth]{Louisville}
	\end{center}
\end{frame}

\begin{frame}{The sample mean}
	\begin{itemize}
		\item Sally says that the average high temperature in Louisville, Kentucky in September is between 84 and 84.5. Paul says that the average is between 0 and 200 degrees.
		\item[]
		\item Who is more ``right'' if the true (population) value is 82 degrees?
		\item[]
		\item Paul wins! \pause
		\begin{itemize}
			\item This is silly
			\item Paul's interval estimate does contain the true population mean but gives us practically no information
			\item This highlights the complexity of using the sample mean to estimate a population mean
		\end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}{The sample mean}{Confidence intervals}
	\begin{center}
		\includegraphics[width=.8 \linewidth]{point1}
	\end{center}
\begin{itemize}
	\item For continuous random variables the probability that the sample mean is equal to the population mean is zero (for a finite sample size $n$) \pause 
	\item[]
	\item Unfortunately the picture above would never happen
\end{itemize}
\end{frame}

\begin{frame}{The sample mean}{Confidence intervals}
	\begin{center}
		\includegraphics[width=.8 \linewidth]{point2}
	\end{center}
	\begin{itemize}
		\item For continuous random variables the probability that the sample mean is equal to the population mean is zero (for a finite sample size $n$)
		\item[]
		\item Instead we have something like the above picture
	\end{itemize}
\end{frame}

\begin{frame}{The sample mean}{Confidence intervals}
	\begin{center}
		\includegraphics[width=.8 \linewidth]{point3}
	\end{center}
	\begin{itemize}
		\item With the above picture we want to propose an interval like the blue line between blue dots that is likely to contain the population mean \pause 
		\item The most straightforward interval would be to make an interval that is centered on the sample mean but extends a little to the left and a little to the right \pause 
		\item But how far to the left and right?!?!
	\end{itemize}
\end{frame}

\begin{frame}{The sample mean}{Confidence intervals}
	\begin{center}
		\includegraphics[width=.8 \linewidth]{point3}
	\end{center}
	\begin{itemize}
		\item But how far to the left and right?!?!
		\item We want an interval that is wide enough to contain the population mean \emph{most of the time}*, but not so wide that it has little information** \\
		\vspace{2.5pt}
		{\tiny *Sally's interval was bad because it doesn't contain the population mean
			**Paul's interval, while trivially containing the population mean, didn't really inform us where the population mean was likely to be}
	\end{itemize}
\end{frame}

\begin{frame}{The sample mean}{Confidence intervals}
	\begin{itemize}
		\item We want to make \textbf{\emph{interval estimates}} for a population mean $\mu$ such that for a fixed \textbf{\emph{level of confidence}}, called $1-\alpha$,
		we expect that the population mean will be contained in the interval estimate $(1-\alpha) * 100 \%$ of the time \pause 
		\item[]
		\item For example, if we fix the level of confidence at .95, we expect that 95\% of the confidence intervals that we construct by sampling from a population will contain the population mean $\mu$. In this case $\alpha = .05$
	\end{itemize}
\end{frame}

\begin{frame}{The sample mean}{Confidence intervals}
	\begin{center}
		\includegraphics[width = .9\linewidth]{coverage1}
	\end{center}
\end{frame}

\begin{frame}{The sample mean}{Confidence intervals}
	Why we say that ``we expect that the population mean will be contained in the interval estimate 95\% of the time''
	\begin{center}
		\includegraphics[width = .85\linewidth]{coverage2}
	\end{center}
\end{frame}

\begin{frame}{The sample mean}{Confidence intervals}
	Why we say that ``we expect that the population mean will be contained in the interval estimate 95\% of the time''
	\begin{center}
		\includegraphics[width = .85\linewidth]{coverage3}
	\end{center}
\end{frame}

\subsection{Confidence intervals for $\mu$ given known $\sigma$}
\begin{frame}{Outline}
	\tableofcontents[currentsection,subsectionstyle=show/shaded/hide]
\end{frame}

\begin{frame}{The sample mean}{Confidence intervals}
	\begin{itemize}
		\item To construct confidence intervals for the population mean, $\mu$, for a fixed confidence level of $\alpha$ if $y$ has a normal distribution with known $\sigma$: \pause 
		\begin{enumerate}
			\item Compute the sample mean: $\bar{y}$ \pause 
			\item[]
			\item Compute the lower confidence interval endpoint: $\bar{y} - z_{\alpha / 2} \times \sigma / \sqrt{n}$ \pause 
			\item[]
			\item Compute the upper confidence interval endpoint: $\bar{y} + z_{\alpha / 2} \times \sigma / \sqrt{n}$
		\end{enumerate}
	\item[]
	\item \#2 and \#3 are often written together as $\bar{y} \pm z_{\alpha / 2} \times \sigma / \sqrt{n}$
	\end{itemize}
\end{frame}

\begin{frame}{Confidence intervals for $\mu$}
		\begin{itemize}
			\item Suppose we knew the weight of every fish in the pond by the horseshoe. This group of fish would constitute a \textbf{population} \pause 
			\item The weights of the fish are: \\
			498.64 499.59 510.11 498.42 478.43 504.99 492.45 507.79 507.55 489.00 501.67 499.71 518.76 502.45 507.02 499.85
			498.57 503.21 501.22 494.05 495.58 502.91 507.24 504.60 501.85 502.34 505.93 520.01 481.63 491.38 515.83 501.55
			497.25 507.88 497.77 513.92 495.11 501.37 500.04 492.73 492.79 498.09 513.35 503.56 508.43 507.75 500.80 493.27
			518.36 497.93 494.60 484.99 502.68 500.34 490.05 492.30 494.22 490.67 478.23 494.86 500.89 502.94 506.92 483.88
			501.88 500.77 498.26 486.01 496.31 504.51 516.26 519.23 498.37 495.61 511.91 516.70 511.33 484.83 527.31 504.99
			485.08 509.22 494.83 521.05 491.13 498.32 509.09 486.05 508.14 490.43 496.30 514.79 493.96 497.09 482.16 522.58
			493.33 504.78 488.39 496.20
		\end{itemize}
\end{frame}

\begin{frame}{Confidence intervals for $\mu$}
	\begin{itemize}
		\item Example problem 1: Suppose you have the following sample from the fish weights data: $\{501.85, 495.58, 502.94, 508.43, 511.33 \}$ and that the population standard deviation is known, $\sigma = 10.028$. Compute a 95\% confidence interval for the population mean  \pause
		\begin{gather*}
			\bar{y} = 504.026 \\
			1 - \alpha = 0.95 \\
			\alpha = 0.05 \\
			\alpha / 2 = 0.025 \\
			z_{\alpha / 2} = z_{0.025} = 1.96 
		\end{gather*} \pause
		So the lower endpoint is:  \pause
		\begin{align*}
			\bar{y} - z_{\alpha / 2} \times \sigma / \sqrt{n} &= 504.026 - (1.96)(10.028)/\sqrt{5} \\
			&= 504.026 - 8.790 \\
			&= 495.236
		\end{align*}
	\end{itemize}
\end{frame}

\begin{frame}{Confidence intervals for $\mu$}
	\begin{center}
		\includegraphics[width=1\linewidth]{newTables}
	\end{center}
\end{frame}

\begin{frame}{Confidence interval for $\mu$}
	\begin{itemize}
		\item Example problem 1: Suppose you have the following sample from the fish weights data: $\{501.85, 495.58, 502.94, 508.43, 511.33 \}$ and that the population standard deviation is known, $\sigma = 10.028$. Compute a 95\% confidence interval for the population mean\\ \vspace{10pt}
		
		And the upper endpoint is:  \pause
		\begin{align*}
		\bar{y} + z_{\alpha / 2} \times \sigma / \sqrt{n} &= 504.026 + (1.96)(10.028)/\sqrt{5} \\
		&= 504.026 + 8.790 \\
		&= 512.816
		\end{align*}  \pause
		
		So the 95\% confidence interval for the population mean is $(495.236, 512.816)$
	\end{itemize}
\end{frame}

\begin{frame}{$\alpha$ versus $1-\alpha$}
	\begin{itemize}
		\item Up until this point we have been working with left-tailed probabilities of the standard normal distribution, $P(Z \leq z)$:
		\begin{center}
			\includegraphics[width=.5\linewidth]{standardNormal}
		\end{center}  \pause
		\item And we used $z_c$ to mean the $c$\textsubscript{th} percentile of the standard normal distribution  \pause
		\item[]
		\item For example, $z_{.05} = -1.645$  \pause
		\item[]
		\item Now we will use $z_{\alpha}$ to denote right tailed probabilities, for example $z_{.05}=P(Z\geq .95) = 1.645$
	\end{itemize}
\end{frame}

\begin{frame}{$\alpha$ versus $1-\alpha$}
	\begin{center}
		\includegraphics[width=1\linewidth]{newTables}
	\end{center}
\end{frame}

\begin{frame}{Confidence intervals for $\mu$}
	\begin{itemize}
		\item Example problem 2: Suppose you have the following sample from the fish weights data: $\{501.85, 495.58, 502.94, 508.43, 511.33 \}$ and that the population standard deviation is known, $\sigma = 10.028$. Compute a 90\% confidence interval and a 99\% confidence interval for the population mean\\  \vspace{10pt}  \pause
		
		For the 90\% confidence interval, $\alpha = .10$ and 
		\begin{gather*}
			z_{\alpha / 2} = z_{0.05} = 1.645
		\end{gather*} \pause
		For the 99\% confidence interval, $\alpha = .01$ and 
		\begin{gather*}
		z_{\alpha / 2} = z_{0.005} = 2.576
		\end{gather*}  \pause
		
		\item For both the 90\% and 99\% confidence intervals, the other values, such as $\bar{y}$, $\sigma$, and $n$ will remain as before
	\end{itemize}
\end{frame}

\begin{frame}{Confidence intervals for $\mu$}
	\begin{itemize}
		
		\item For the 90\% confidence interval: 
		\begin{align*}
			\bar{y} &\pm z_{\alpha / 2} \times \sigma / \sqrt{n} \\
			504.026 &\pm (1.645)(10.028)/\sqrt{5} \\
			(496.649&, 511.403)
		\end{align*}  \pause
		
		\item For the 99\% confidence interval: 
		\begin{align*}
		\bar{y} &\pm z_{\alpha / 2} \times \sigma / \sqrt{n} \\
		504.026 &\pm (2.576)(10.028)/\sqrt{5} \\
		(492.474&, 515.578)
		\end{align*}
		
	\end{itemize}
\end{frame}

\begin{frame}{Confidence intervals for $\mu$}{Width of the interval}
	\begin{itemize}
		\item Here are the three confidence intervals we have computed:
		\begin{itemize}
			\item 90\%: $(496.649, 511.403)$
			\item 95\%: $(495.236, 512.816)$
			\item 99\%: $(492.474, 515.578)$
		\end{itemize}  \pause
	\item[]
	\item Question \#1: How does the width of the confidence interval change as you increase the confidence level $1-\alpha$?  \pause
	\item[]
	\item Question \#2: How would the width of a confidence interval change (for a fixed $\sigma$ and $\alpha$) as $n$ is increased?
	\end{itemize}
\end{frame}

\begin{frame}{Confidence intervals for $\mu$}{Width of the interval}
	\begin{itemize}
		\item Question \#2: How would the width of a confidence interval change (for a fixed $\sigma$ and $\alpha$) as $n$ is increased?  \pause
		\item[]
		\item We have both intuition and a mathematical solution:
		\begin{itemize}
			\item Intuition: If we have more measurements from a distribution, we should have a more precise estimates of a population parameter $=$ smaller interval  \pause
			\item[]
			\item Mathematical solution: Let $n_2 > n_1$, then:
			\begin{gather*}
				\sqrt{n_2} > \sqrt{n_1} \text{ and } \frac{\sigma}{\sqrt{n_2}} < \frac{\sigma}{\sqrt{n_1}} \\
				\text{so...} \\
				z_{\alpha / 2} \frac{\sigma}{\sqrt{n_2}} < z_{\alpha / 2} \frac{\sigma}{\sqrt{n_1}} \\
				width_{n_2} < width_{n_1}
			\end{gather*}
		\end{itemize}
	\end{itemize}
\end{frame}

\subsection{Confidence intervals for $\mu$ when $\sigma$ is unknown}

\begin{frame}{Outline}
	\tableofcontents[currentsection,subsectionstyle=show/shaded/hide]
\end{frame}

\begin{frame}{Confidence intervals for $\mu$}{$\sigma$ unknown}
\begin{columns}
	\begin{column}{.4 \textwidth}
		\begin{center}
			\includegraphics[width=1.2 \linewidth]{./../lecture1_Introduction/Sampling.png}
		\end{center}
	\end{column}
	\begin{column}{.5 \textwidth}
		\begin{itemize}
			\item When we draw a sample from a population in order to estimate the population mean $\mu$ of a normal random variable, the vast majority of the time we don't know the population standard deviation $\sigma$  \pause
			\item[]
			\item It seem sensible that we can estimate $\sigma$ with $s$, the sample standard deviation, and use this value when we need it (e.g. to construct confidence intervals)
		\end{itemize}
	\end{column}
\end{columns}
\end{frame}

\begin{frame}{The Student's t-distribution}
	\begin{columns}
		\begin{column}{.5 \textwidth}
			\includegraphics[width=1 \linewidth]{guiness}
		\end{column}
		\begin{column}{.5 \textwidth}
			\includegraphics[width=.8 \linewidth]{William_Sealy_Gosset}
			William Sealy Gosset (Head Experimental Brewer at Guinness)
		\end{column}
	\end{columns}
\end{frame}

\begin{frame}{Confidence intervals for $\mu$}{$\sigma$ unknown}
	\begin{itemize}
		\item William Sealy Gosset knew that confidence intervals for estimating $\mu$ constructed using $s$ instead of $\sigma$ systematically do not contain $\mu$ as often as expected  \pause
		\item[]
		\item He proposed using a different type of distribution for computing the width of confidence intervals  \pause
		\begin{itemize}
			\item Instead of using $z_{\alpha / 2}$, where $z$ is the value from the standard normal distribution, use $t_{\alpha / 2, n - 1}$ where $t$ is a value from a ``Student's t-distribution'' with $n-1$ ``degrees of freedom''  \pause
			\item We will talk about the Student's t-distribution in more detail later in the lecture
				\item[]  \pause
		\end{itemize}
			\item Confidence intervals for $\mu$ if $\sigma$ is unknown: $\bar{y} \pm t_{\alpha / 2, n - 1} \times s / \sqrt{n}$
	\end{itemize}
\end{frame}

\begin{frame}{Student's $t$-distribution}
	Find the value $t_{\alpha / 2, df}$ if $\alpha = .05$ and $df = 4$. 
	\begin{center}
		\includegraphics[width = .8\linewidth]{tTable}
	\end{center}
\end{frame}

\begin{frame}{Confidence intervals for $\mu$}{$\sigma$ unknown}
	\begin{itemize}
		\item Example problem 1: Suppose you have the following sample from the fish weights data: $\{501.85, 495.58, 502.94, 508.43, 511.33\}$. Calculate a 95\% confidence interval for $\mu$ using the given sample (we don't know the population $\sigma$)  \pause \\
		\vspace{10 pt}
		First we note that $\alpha = 0.05$, so $\alpha / 2 = 0.025$; and that $n - 1 = 4$  \pause So the $t$-value that we are looking for is:
		\begin{gather*}
			t_{\alpha / 2, n - 1} = t_{0.025, 4} = 2.776
		\end{gather*}  \pause
		From before we have that $\bar{y}=504.026$. Then we compute the sample standard deviation, $s = 6.124$. We are almost there:  \pause
		\begin{align*}
			\bar{y} &\pm t_{\alpha / 2, n - 1} \times s / \sqrt{n} \\
			504.026 &\pm 2.776 \times 6.124 / 2.236 \\
			(496.423&, 511.629)
		\end{align*}
	\end{itemize}
\end{frame}

\section{Tests of hypotheses regarding Normal population mean}

\subsection{Tests given a known $\sigma$}
\begin{frame}{Outline}
	\tableofcontents[currentsection,subsectionstyle=show/shaded/hide]
\end{frame}

\begin{frame}{Tests regarding Normal population means}
	\begin{itemize}
		\item Up until now we have been discussing how to use a random sample to estimate the value of $\mu$, the population mean of random variable that has a Normal distribution  \pause
		\begin{itemize}
			\item We used $\bar{y}$ as a point estimate for $\mu$, and $\bar{y} \pm z_{\alpha / 2} \times \sigma / \sqrt{n}$ or $\bar{y} \pm t_{\alpha / 2, n - 1} \times s / \sqrt{n}$ as an interval estimate
			\item[]
		\end{itemize}
	\item Now imagine that we want to test whether $\mu$ is less than, equal to, or greater than some pre-specified value $\mu_0$  \pause
	\item[]
	\item Example: I want to test whether mean the population mean of systolic blood pressure in Asian-Americans is less than 133.5 (the mean in Caucasian Americans in a large study)
	
	\end{itemize}
\end{frame}

\begin{frame}{Process}
	The process for testing hypotheses regarding a Normal population mean is: 
	\begin{enumerate}
		\item Define a ``research hypothesis'' / alternative hypothesis. We denote this $H_a$  \pause
		\item[]
		\item Determine the appropriate null hypothesis. We denote this $H_0$  \pause
		\item[]
		\item Using a simple random sample, we compute a test statistic  \pause
		\item[]
		\item We evaluate whether the test statistic supports the rejection of the null hypothesis
	\end{enumerate}
\end{frame}

\begin{frame}{Process}{Example}
Example: We want to test whether the population mean of systolic blood pressure (SBP) in Asian-Americans is less than 133.5\\
\vspace{10 pt}
	\begin{enumerate}
		\item Define a ``research hypothesis'' / alternative hypothesis. We denote this $H_a$  \pause
		\begin{itemize}
			\item $H_a$: The population mean of SBP is less than 133.5. $\mu < 133.5$  \pause
		\end{itemize}
		\item Determine the appropriate null hypothesis. We denote this $H_0$   \pause
		\begin{itemize}
			\item $H_0$: The population mean of SBP is greater than or equal to 133.5. $\mu \geq 133.5$  \pause
		\end{itemize}
		\item Using a simple random sample, we compute a test statistic  \pause
		\item[]
		\item We evaluate whether the test statistic supports the rejection of the null hypothesis
	\end{enumerate}
\end{frame}

\begin{frame}{Process}
	\begin{itemize}
		\item We now need a \textbf{\emph{test statistic}} that will inform us whether we have evidence from the sample to reject the null hypothesis  \pause
		\item[]
		\item For normally distributed random variables, we will use the sample mean $\bar{y}$ as the test statistic  \pause
		\item[] 
		\item So how can our test statistic $\bar{y}$ inform us whether we have evidence from the sample to reject the null hypothesis?
	\end{itemize}
\end{frame}

\begin{frame}{Distribution of test statistics}
	\begin{itemize}
		\item We know what the distribution of test statistics is if the null hypothesis is true, that is $\mu = \mu_0$  \pause
	\end{itemize}
\begin{center}
	\includegraphics[width = .95 \linewidth]{nullTests}
\end{center}
\end{frame}

\begin{frame}{Distribution of test statistics}
	\begin{itemize}
		\item So when a test statistic is observed that is inconsistent with the null hypothesis we will see it:
	\end{itemize}
	\begin{center}
		\includegraphics[width = .95 \linewidth]{nullTests2}
	\end{center}
\end{frame}

\begin{frame}{Distribution of test statistics}
	\begin{itemize}
		\item We define rejection regions based on our hypotheses:
	\end{itemize}
	\begin{center}
		\includegraphics[width = .9 \linewidth]{nullTests3}
	\end{center}
\end{frame}

\begin{frame}{Distribution of test statistics}
	\begin{itemize}
		\item We reject a null hypothesis if the test statistic is in the rejection region
	\end{itemize}
	\begin{center}
		\includegraphics[width = .85 \linewidth]{nullTests4}
	\end{center}
\end{frame}

\begin{frame}{Distribution of test statistics}
	\begin{itemize}
		\item We don't reject a null hypothesis if the test statistic is in the acceptance region
	\end{itemize}
	\begin{center}
		\includegraphics[width = .85 \linewidth]{nullTests5}
	\end{center}
\end{frame}

\begin{frame}{Rejection regions}{Types of errors}
	\begin{itemize}
		\item \textbf{\emph{Type I error:}} A Type I error is commited if we reject the null hypothesis when it is true. This is denoted $\alpha$  \pause
		\item[]
		\item \textbf{\emph{Type II error:}} A Type II error is commited if we accept the null hypothesis when it is false and the alternative hypothesis is true. This is denoted $\beta$  \pause
	\end{itemize}
\begin{center}
	\begin{tabular}{c|cc}
		& \textbf{Null hypothesis} & \\
		\textbf{Decision} & \textbf{True} & \textbf{False} \\ \hline
		Reject $H_0$ & Type I error & Correct \\
		& $\alpha$ & $1-\beta$ \\ \hline
		Accept $H_0$ & Correct & Type II error \\
		& $1-\alpha$ & $\beta$ \\ \hline
	\end{tabular}
\end{center}
\end{frame}

\begin{frame}{Rejection regions}
	\begin{center}
				\includegraphics[width = .65 \linewidth]{nullTests3b}
	\end{center}
\begin{itemize}
	\item We define rejection regions based on fixing the Type I error rate $\alpha$ at a level that is ``acceptable'' such as $\alpha = 0.05$  \pause
	\item If $H_a$: $\mu < \mu_0$ this would correspond to a rejection region in the lower tail with an area of 0.05
\end{itemize}
\end{frame}

\begin{frame}{Rejection regions}
	\begin{center}
		\includegraphics[width = .65 \linewidth]{nullTests3Flipb}
	\end{center}
	\begin{itemize}
		\item We define rejection regions based on fixing the Type I error rate $\alpha$ at a level that is ``acceptable'' such as $\alpha = 0.05$  \pause
		\item If $H_a$: $\mu > \mu_0$ this would correspond to a rejection region in the upper tail with an area of 0.05
	\end{itemize}
\end{frame}

\begin{frame}{Rejection regions}
	\begin{center}
		\includegraphics[width = .65 \linewidth]{nullTests6b}
	\end{center}
	\begin{itemize}
		\item We define rejection regions based on fixing the Type I error rate $\alpha$ at a level that is ``acceptable'' such as $\alpha = 0.05$  \pause
		\item If $H_a$: $\mu = \mu_0$ this would correspond to a rejection region in the lower tail with an area of 0.025 and in the upper tail with an area of 0.025
	\end{itemize}
\end{frame}

\begin{frame}{Test statistic}
	\begin{itemize}
		\item Earlier we mentioned that the test statistics we use for testing hypotheses regarding a Normal population mean, $\mu$, is the sample mean $\bar{y}$  \pause
		\item[]
		\item This is true, however, we want to standardize the test statistic so that we can use a standard normal distribution in our calculations:
		\begin{gather*}
			z = \frac{\bar{y} - \mu_0}{\sigma / \sqrt{n}}
		\end{gather*}
	\end{itemize}
\end{frame}

\begin{frame}{Process}
	\begin{itemize}
		\item Hypotheses:  \pause
		\begin{itemize}
			\item Case 1. $H_0: \mu \leq \mu_0$ vs $H_a: \mu > \mu_0$ (upper/right-tailed test) \pause
			\item Case 2. $H_0: \mu \geq \mu_0$ vs $H_a: \mu < \mu_0$ (lower/left-tailed test) \pause
			\item Case 3. $H_0: \mu = \mu_0$ vs $H_a: \mu \neq \mu_0$ (two-tailed test) \pause
		\end{itemize}
		\item[]
		\item Test statistic:
		\begin{gather*}
					z = \frac{\bar{y} - \mu_0}{\sigma / \sqrt{n}}
		\end{gather*} \pause
		
		\item For a fixed $\alpha$, the rejection region is:
		\begin{itemize}
			\item Case 1. Reject $H_0$ if $z \geq z_{\alpha}$  \pause
			\item Case 2. Reject $H_0$ if $z \leq -z_{\alpha}$  \pause
			\item Case 3. Reject $H_0$ if $|z| \geq z_{\alpha/2}$ 
		\end{itemize}
		
	\end{itemize}
\end{frame}

\begin{frame}{Example}
	Example: You want to test whether the population mean of systolic blood pressure (SBP) in Asian-Americans is less than 133.5. \\
	\vspace{10 pt}
	\begin{enumerate}
		\item Define a ``research hypothesis'' / alternative hypothesis. We denote this $H_a$
		\begin{itemize}
			\item $H_a$: The population mean of SBP is less than 133.5. $\mu < 133.5$
		\end{itemize}
		\item Determine the appropriate null hypothesis. We denote this $H_0$
		\begin{itemize}
			\item $H_0$: The population mean of SBP is greater than or equal to 133.5. $\mu \geq 133.5$
		\end{itemize}
		\item Using a simple random sample, we compute a test statistic
		\item[]
		\item We evaluate whether the test statistic supports the rejection of the null hypothesis
	\end{enumerate}
\end{frame}

\begin{frame}{Example}
	\textbf{Example:} You want to test whether the population mean of systolic blood pressure (SBP) in Asian-Americans is less than 133.5. Assume $\sigma = 20$ is known. You recruit 800 participants and measure their SBP. You find that the sample mean is 135.5. \\
	\vspace{5pt}
	
	We want to test $H_a$ vs $H_0$ by seeing if our test statistic is in the rejection region (lower-tail). Let's pre-specify $\alpha = 0.05$ as acceptable. First we compute the test statistic:
	\begin{align*}
	z &= \frac{\bar{y} - \mu_0}{\sigma / \sqrt{n}} \\
	z &= \frac{135.5 - 133.5}{20 / \sqrt{800}} \\
	z &= 2.828
	\end{align*}
\end{frame}

\begin{frame}{Example}
	\textbf{Example:} You want to test whether the population mean of systolic blood pressure (SBP) in Asian-Americans is less than 133.5. Assume $\sigma = 20$ is known. You recruit 800 participants and measure their SBP. You find that the sample mean is 135.5 \\ 
	\vspace{15pt}  
	
	The rejection region will be to the left of $-z_{\alpha} = -z_{.05} = -1.645$.
\end{frame}

\begin{frame}{Example}
	\begin{center}
		\includegraphics[width = .8\linewidth]{SBP1}
	\end{center}

\vspace{-10pt}
Clearly, 2.828 is not less than -1.645; the test statistic is not in the rejection region. We fail to reject the null hypothesis
\end{frame}

\begin{frame}{Example 2}
	\begin{itemize}
		\item Research question: Cells spend on average 16.7\% of their life in G2 phase (so we expect that in a large population of cells 16.7\% will be in this phase). The population standard deviation is 9\%. You want to know if the \% of cells in G2 phase is abnormal (different from 16.7\%) in prostate cancer tumors. You measured the \% of cells in G2 phase from 139 tumors and found a sample mean of 14.275. You decide that you will test at $\alpha = 0.10$  \pause
		\item[]
		\item Hypotheses:  \pause
		\begin{itemize}
			\item $H_0: \mu = 16.7$  \pause
			\item $H_A: \mu \neq 16.7$  \pause
		\end{itemize}
	\item[]
	\item Rejection region: Reject $H_0$ if $|z| \geq z_{\alpha / 2} = z_{.05} = 1.645$
	\end{itemize}
\end{frame}

\begin{frame}{Example 2}
	\begin{itemize}
		\item Research question: Cells spend on average 16.7\% of their life in G2 phase (so we expect that in a large population of cells 16.7\% will be in this phase). The population standard deviation is 9\%. You want to know if the \% of cells in G2 phase is abnormal (different from 16.7\%) in prostate cancer tumors. You measured the \% of cells in G2 phase from 139 tumors and found a sample mean of 14.275. You decide that you will test at $\alpha = 0.10$.
		\begin{align*}
			z &= \frac{\bar{y} - \mu_0}{\sigma / \sqrt{n}} \\
			&= \frac{14.275 - 16.7}{9 / \sqrt{139}} \\
			&= -3.177
		\end{align*}
	\end{itemize}
\end{frame}

\begin{frame}{Example 2}
	\begin{itemize}
		\item Research question: Cells spend on average 16.7\% of their life in G2 phase (so we expect that in a large population of cells 16.7\% will be in this phase). The population standard deviation is 9\%. You want to know if the \% of cells in G2 phase is abnormal (different from 16.7\%) in prostate cancer tumors. You measured the \% of cells in G2 phase from 139 tumors and found a sample mean of 14.275. You decide that you will test at $\alpha = 0.10$.
		
		\item[]
		
		\item Since $|-3.177| \geq 1.645$, we have that $|z| \geq z_{\alpha / 2}$, and we do reject the null hypothesis. We have evidence to conclude that the \% of cells in G2 phase is abnormal in prostate cancer tumors. 
	\end{itemize}
\end{frame}

\subsection{Level of significance}
\begin{frame}{Outline}
\tableofcontents[currentsection,subsectionstyle=show/shaded/hide]
\end{frame}

\begin{frame}{Level of significance}
	\begin{itemize}
		\item \textbf{\emph{Level of significance:}} The probability of obtaining a value of the test statistic that would have been as likely or more likely to have led to the rejection of $H_0$ as the observed value, given that $H_0$ is true. Also called the \textbf{\emph{p-value}} for the test  \pause
		\item[]
		\item We are about to see some example pictures from a test of $H_0: \mu > \mu_0$ versus $H_a: \mu \leq \mu_0$. In this example we did observe a test statistic that is in the lower-tail of the distribution. We want to think about how rare this observed test statistic is if the null hypothesis is true
	\end{itemize}
\end{frame}

\begin{frame}{Level of significance}
	\begin{center}
		\includegraphics[width=1 \linewidth]{rare1}
	\end{center}
\end{frame}

\begin{frame}{Level of significance}
\begin{center}
	\includegraphics[width=1 \linewidth]{rare2}
\end{center}
\end{frame}

\begin{frame}{Level of significance}
\begin{center}
	\includegraphics[width=1 \linewidth]{rare3}
\end{center}
\end{frame}

\begin{frame}{Level of significance}
\begin{center}
	\includegraphics[width=.6 \linewidth]{rare3}
\end{center}
\vspace{-10pt}
\begin{itemize}
	\item \textbf{\emph{Level of significance:}} The probability of obtaining a value of the test statistic that would have been as likely or more likely to have led to the rejection of $H_0$ as the observed value, given that $H_0$ is true  \pause
	\item In this case, the level of significance is roughly $1/10000$ or $0.0001$
\end{itemize}
\end{frame}

\begin{frame}{Level of significance}{Back to SBP example}
\begin{itemize}
	\item Let's test $H_0: SBP > 133.5$ versus $H_a: SBP \leq 133.5$  \pause
	\item[]
	\item The sample mean is $135.5$ from 800 measurements and the population $\sigma = 20$  \pause
	\item[]
	\item Let's compute the test statistic:
	\begin{align*}
	z &= \frac{\bar{y} - \mu_0}{\sigma / \sqrt{n}} = \frac{135.5 - 133.5}{20 / \sqrt{800}} = 2.828
	\end{align*}
\end{itemize}
\end{frame}

\begin{frame}{Level of significance}{Back to SBP example}
\begin{itemize}
	\item Our test statistic TS is $z = 2.828$. $P(Z \geq 2.828) = 0.002$. $p$-value = 0.002  \pause
\end{itemize}
\vspace{-10pt}
\begin{center}
	\includegraphics[width = .7\linewidth]{sig1}
\end{center}
\end{frame}

\begin{frame}{Level of significance}
	\begin{itemize}
		\item Case 1:  \pause
		\begin{itemize}
			\item $H_0: \mu \leq \mu_0$  \pause
			\item $H_a: \mu > \mu_0$  \pause
			\item $p$-value: $P(z \geq TS)$  \pause
		\end{itemize}
		\item[]
		\item Case 2:
		\begin{itemize}
			\item $H_0: \mu \geq \mu_0$  \pause
			\item $H_a: \mu < \mu_0$  \pause
			\item $p$-value: $P(z \leq TS)$  \pause
		\end{itemize}
		\item[]
		\item Case 3:
		\begin{itemize}
			\item $H_0: \mu = \mu_0$ \pause
			\item $H_a: \mu \neq \mu_0$ \pause
			\item $p$-value: $2\times P(z \geq |TS|)$ 
		\end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}{P-values for two sided tests}
\begin{itemize}
	\item When we conduct two-sided tests there are two ways we can observe test statistics as extreme as ours given the null hypothesis is true
	\item $p$-value: $2\times P(z \geq |TS|)$ 
\end{itemize}
\vspace{-5 pt}
\begin{center}
	\includegraphics[width = .7 \linewidth]{sig2}
\end{center}
\end{frame}

\begin{frame}{Level of significance}{Back to Cells in G2 phase example}
		\begin{itemize}
		\item Research question: Cells spend on average 16.7\% of their life in G2 phase (so we expect that in a large population of cells 16.7\% will be in this phase). The population standard deviation is 9\%. You want to know if the \% of cells in G2 phase is abnormal (different from 16.7\%) in prostate cancer tumors. You measured the \% of cells in G2 phase from 139 tumors and found a sample mean of 14.275  \pause
		
		\item[]
		
		\item Recall that we tested $H_0: \mu = \mu_0$ versus $H_a: \mu \neq \mu_0$ and that our test statistic was $z = -3.177$  \pause
		
		\item[]
		
		\item $p$-value: $2\times P(z \geq |TS|) = 2 \times P(z \geq 3.177) = 2 (0.0007) = 0.0014$ 
	\end{itemize}
\end{frame}

\begin{frame}{Decision rules}
	\begin{itemize}
		\item Most of the time, we want to fix our Type I error rate ($\alpha$) before doing any statistical testing  \pause
		
		\item[]
		
		\item We then can use decision rules for determining whether or not to reject a null hypothesis  \pause
		
		\item[]
		
		\item Decision rules:  \pause
		\begin{enumerate}
			\item If $p$-value $\leq \alpha$, then we reject $H_0$  \pause
			\item If $p$-value $\geq \alpha$, then we fail to reject $H_0$
		\end{enumerate}
	\end{itemize}
\end{frame}

\begin{frame}{Words of caution about $p$-values}
	\begin{itemize}
		\item In the sciences, many forms of $p$-value abuse are ubiquitous  \pause
		\item[]
		\item \textbf{Abuse \#1:} Confusing practical significance with level of significance. Consider three test statistics from different size samples:  \pause
		\begin{align*}
		z_{n=800} &= \frac{\bar{y} - \mu_0}{\sigma / \sqrt{n}} = \frac{135.5 - 133.5}{20 / \sqrt{800}} = 2.828 \\
		z_{n=8} &= \frac{\bar{y} - \mu_0}{\sigma / \sqrt{n}} = \frac{135.5 - 133.5}{20 / \sqrt{8}} = 0.283 \\
		z_{n=8000} &= \frac{\bar{y} - \mu_0}{\sigma / \sqrt{n}} = \frac{135.5 - 133.5}{20 / \sqrt{8000}} = 8.944
		\end{align*}  \pause
		These yield the following $p$-values:  \pause
		\begin{align*}
			P(Z >z_{n=800}) &= 0.002 \\
			P(Z >z_{n=8}) &= 0.39 \\
			P(Z >z_{n=8000}) &= 0.0000000000000000002
		\end{align*}
	\end{itemize}
\end{frame}

\begin{frame}{Words of caution about $p$-values}
\begin{itemize}
	\item These yield the following $p$-values:
	\begin{align*}
	P(Z >z_{n=800}) &= 0.002 \\
	P(Z >z_{n=8}) &= 0.39 \\
	P(Z >z_{n=8000}) &= 0.0000000000000000002
	\end{align*}
	So the level of significance is very different. However, in each case the difference between 135.5 and 133.5 is 2.0. Whether 2 mmHg is a ``significant difference'' from the null value is a question for a medical doctor or biomedical scientist not a statistician
\end{itemize}
\end{frame}

\begin{frame}{Words of caution about $p$-values}
	\begin{itemize}
		\item \textbf{Abuse \#2:} The choice of $\alpha = 0.05$ is completely arbitrary, so $p$-value based decision rules are as well  \pause
		\begin{itemize}
			\item Imagine your chances of winning the lottery go from 4.9\% (today) to 5.1\% (tomorrow). Should you change your decision to play the lottery based on this improvement? Do you feel like it makes a difference?  \pause
			\item Rejecting a null hypothesis if the $p$-value is 0.049 but not 0.051 is like this lottery decision
		\end{itemize}
	\end{itemize}
\end{frame}

\subsection{Tests given $\sigma$ is unknown}
\begin{frame}{Outline}
\tableofcontents[currentsection,subsectionstyle=show/shaded/hide]
\end{frame}

\begin{frame}{The Student's t-distribution}
\begin{columns}
	\begin{column}{.5 \textwidth}
		\includegraphics[width=1 \linewidth]{guiness}
	\end{column}
	\begin{column}{.5 \textwidth}
		\includegraphics[width=.8 \linewidth]{William_Sealy_Gosset}
		
		William Sealy Gosset (Head Experimental Brewer at Guinness)
	\end{column}
\end{columns}
\end{frame}

\begin{frame}{The Student's t-distribution}
	\begin{itemize}
		\item Gosset's great discovery: Gosset realized that when he used the test statistic $z = \frac{\bar{y}-\mu_0}{\sigma / \sqrt{n}}$, with $\sigma$ being replaced by $s$, the null hypothesis was falsely rejected at a higher rate than a specified $\alpha$  \pause
		\item[]
		\item Gosset derived the correct distribution for the test statistic $z = \frac{\bar{y}-\mu_0}{s / \sqrt{n}}$  \pause
		\item[]
		\item Since scientists at Guinness were not allowed to publish their work, he published it under the pseudonym ``Student'', hence the name ``Student's'' $t$-distribution
	\end{itemize}
\end{frame}

\begin{frame}{The Student's t-distribution}{Facts}
\begin{enumerate}
	\item There are many different $t$ distributions. The parameter ``degrees of freedom'' (df) is what determines what the distribution looks like  \pause
	\item[]
	\item The $t$ distribution is symmetric (about 0)  \pause
	\item[]
	\item Each $t$ distribution has variance $df / (df - 2)$, which is greater than the standard normal distribution  \pause
	\item[]
	\item As $df$ increases, the $t$ distribution approaches the standard normal distribution 
\end{enumerate}	
\end{frame}

\begin{frame}{The Student's t-distribution}
	\begin{center}
		\includegraphics[width = .9 \linewidth]{tDists}
	\end{center}
\end{frame}

\begin{frame}{The Student's t-distribution}{Degrees of freedom}
	\begin{itemize}
		\item There are multiple conceptual ways to think about ``degrees of freedom''  \pause
		\item[]
		\item One is that if we have $n$ measurements we have $n$ pieces of information. When we compute the sample mean we ``use up'' one piece of information, so when we compute the sample standard deviation we have $n-1$ pieces of information left
	\end{itemize}
\end{frame}

\begin{frame}{Process}
\begin{itemize}
	\item Hypotheses:  \pause
	\begin{itemize}
		\item Case 1. $H_0: \mu \leq \mu_0$ vs $H_a: \mu > \mu_0$ (upper/right-tailed test)  \pause
		\item Case 2. $H_0: \mu \geq \mu_0$ vs $H_a: \mu < \mu_0$ (lower/left-tailed test)  \pause
		\item Case 3. $H_0: \mu = \mu_0$ vs $H_a: \mu \neq \mu_0$ (two-tailed test)  \pause
	\end{itemize}
	\item[]
	\item Test statistic:  \pause
	\begin{gather*}
	t^* = \frac{\bar{y} - \mu_0}{s / \sqrt{n}}
	\end{gather*}  \pause
	
	\item For a fixed $\alpha$, the rejection region is:
	\begin{itemize}
		\item Case 1. Reject $H_0$ if $t^* \geq t_{\alpha, n-1}$. Significance: $p\text{-value}=P(t \geq t^*)$  \pause
		\item Case 2. Reject $H_0$ if $t^* \leq -t_{\alpha, n-1}$. Significance: $p\text{-value}=P(t \leq t^*)$  \pause
		\item Case 3. Reject $H_0$ if $|t^*| \geq t_{\alpha / 2, n-1}$. Significance: $p\text{-value}=2 \times P(t \geq |t^*|)$
	\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Example}

{\scriptsize  \textbf{Research question:} You want to know if the epidermal growth factor receptor (EGFR) gene expression (or amount) is abnormal in renal cell carcinoma (kidney cancer) tumor tissue. Normal levels in kidney tissue of this gene are 3.34 RPKM (a unit). You have the following 8 measurements of EGFR expression from tumor tissue: $\{4.238, 4.907, 5.459, 4.048, 5.396, 5.230, 5.285, 6.317\}$. Are levels of EGFR expression higher in kidney tumors than normal? Assume you will test at $\alpha = 0.05$} \pause

	\begin{itemize}
		\item First we formulate hypotheses:  \pause
		\begin{itemize}
			\item $H_0: \mu = 3.34$  \pause
			\item $H_a: \mu \neq 3.34$  \pause
			\item[]
		\end{itemize}
	\item Then we compute our test statistic:  \pause
	\begin{itemize}
		\item $\bar{y} = 5.110$  \pause
		\item $s = 0.721$  \pause
		\item $t^* = \frac{5.110 - 3.340}{0.721 / \sqrt{8}} = 6.944$
	\end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}{Example}

{\scriptsize  \textbf{Research question:} You want to know if the epidermal growth factor receptor (EGFR) gene expression (or amount) is abnormal cell carcinoma (kidney cancer) tumor tissue. Normal levels in kidney tissue of this gene are 3.34 RPKM (a unit). You have the following 8 measurements of EGFR expression from tumor tissue: $\{4.238, 4.907, 5.459, 4.048, 5.396, 5.230, 5.285, 6.317\}$. Are levels of EGFR expression higher in kidney tumors than normal? Assume you will test at $\alpha = 0.05$.}

\begin{itemize}
	\item We formulate our rejection rule:   \pause
	\begin{itemize}
		\item Reject $H_0$ if $|t^*| \geq t_{\alpha / 2, n-1}$  \pause
		\item Reject $H_0$ if $|t^*| \geq t_{0.025, 7} = 2.365$  \pause
		\item[] 
	\end{itemize}
	\item We reject the null hypothesis as $|t^*| \geq t_{\alpha / 2, n-1}$ since $6.944 > 2.365$  \pause
	\item[]
	\item We now compute the $p$-value or level of significance: $p\text{-value}=2 \times P(t \geq |t^*|) = 2\times P(t \geq 6.944) = 0.0002$
\end{itemize}
\end{frame}

\begin{frame}{The end of Lecture \# 8}
	\begin{center}
		\includegraphics[width=.9 \linewidth]{DSC_0135_v1}
	\end{center}
\end{frame}

\end{document}